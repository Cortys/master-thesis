% Encoding: UTF-8

@Article{Kipf2017,
  author      = {Thomas N. Kipf and Max Welling},
  title       = {Semi-Supervised Classification with Graph Convolutional Networks},
  journal     = {ICLR},
  year        = {2017},
  abstract    = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  date        = {2016-09-09},
  eprint      = {1609.02907v4},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1609.02907v4:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Misc{Koutra2011,
  author   = {Koutra, Danai and Ramdas, Aaditya and Parikh, Ankur and Xiang, Jing},
  title    = {Algorithms for Graph Similarity and Subgraph Matching},
  year     = {2011},
  abstract = {We deal with two independent but related problems, those of graph similarity and subgraph matching, which are both important practical problems useful in several fields of science, engineering and data analysis. For the problem of graph similarity, we develop and test a new framework for solving the problem using belief propagation and related ideas. For the subgraph matching problem, we develop a new algorithm based on existing techniques in the bioinformatics and data mining literature, which uncover periodic or infrequent matchings. We make substantial progress compared to the existing methods for both problems.},
}

@InCollection{Melnikov2016,
  author    = {Vitalik Melnikov and Eyke Hüllermeier},
  title     = {Learning to Aggregate Using Uninorms},
  booktitle = {Machine Learning and Knowledge Discovery in Databases},
  publisher = {Springer International Publishing},
  year      = {2016},
  pages     = {756--771},
  doi       = {10.1007/978-3-319-46227-1_47},
}

@Article{Narayanan2017,
  author      = {Annamalai Narayanan and Mahinthan Chandramohan and Rajasekar Venkatesan and Lihui Chen and Yang Liu and Shantanu Jaiswal},
  title       = {graph2vec: Learning Distributed Representations of Graphs},
  year        = {2017},
  abstract    = {Recent works on representation learning for graph structured data predominantly focus on learning distributed representations of graph substructures such as nodes and subgraphs. However, many graph analytics tasks such as graph classification and clustering require representing entire graphs as fixed length feature vectors. While the aforementioned approaches are naturally unequipped to learn such representations, graph kernels remain as the most effective way of obtaining them. However, these graph kernels use handcrafted features (e.g., shortest paths, graphlets, etc.) and hence are hampered by problems such as poor generalization. To address this limitation, in this work, we propose a neural embedding framework named graph2vec to learn data-driven distributed representations of arbitrary sized graphs. graph2vec's embeddings are learnt in an unsupervised manner and are task agnostic. Hence, they could be used for any downstream task such as graph classification, clustering and even seeding supervised representation learning approaches. Our experiments on several benchmark and large real-world datasets show that graph2vec achieves significant improvements in classification and clustering accuracies over substructure representation learning approaches and are competitive with state-of-the-art graph kernels.},
  date        = {2017-07-17},
  eprint      = {1707.05005v1},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1707.05005v1:PDF},
  keywords    = {cs.AI, cs.CL, cs.CR, cs.NE, cs.SE},
}

@Article{Grover2016,
  author      = {Aditya Grover and Jure Leskovec},
  title       = {node2vec: Scalable Feature Learning for Networks},
  year        = {2016},
  abstract    = {Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.},
  date        = {2016-07-03},
  eprint      = {1607.00653v1},
  eprintclass = {cs.SI},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1607.00653v1:PDF},
  keywords    = {cs.SI, cs.LG, stat.ML},
}

@InCollection{Adhikari2018,
  author    = {Bijaya Adhikari and Yao Zhang and Naren Ramakrishnan and B. Aditya Prakash},
  title     = {Sub2Vec: Feature Learning for Subgraphs},
  booktitle = {Advances in Knowledge Discovery and Data Mining},
  publisher = {Springer International Publishing},
  year      = {2018},
  pages     = {170--182},
  doi       = {10.1007/978-3-319-93037-4_14},
}

@Misc{Shervashidze2011,
  author   = {Shervashidze, Nino and Schweitzer, Pascal and Leeuwen, Erik Jan Van and Mehlhorn, Kurt and Borgwardt, Karsten M.},
  title    = {Weisfeiler-Lehman Graph Kernels},
  year     = {2011},
  abstract = {In this article, we propose a family of efficient kernels for large graphs with discrete node labels. Key to our method is a rapid feature extraction scheme based on the Weisfeiler-Lehman test of isomorphism on graphs. It maps the original graph to a sequence of graphs, whose node attributes capture topological and label information. A family of kernels can be defined based on this Weisfeiler-Lehman sequence of graphs, including a highly efficient kernel comparing subtree-like patterns. Its runtime scales only linearly in the number of edges of the graphs and the length of the Weisfeiler-Lehman graph sequence. In our experimental evaluation, our kernels outperform state-of-the-art graph kernels on several graph classification benchmark data sets in terms of accuracy and runtime. Our kernels open the door to large-scale applications of graph kernels in various disciplines such as computational biology and social network analysis. },
}

@Article{Kondor2016,
  author      = {Risi Kondor and Horace Pan},
  title       = {The Multiscale Laplacian Graph Kernel},
  abstract    = {Many real world graphs, such as the graphs of molecules, exhibit structure at multiple different scales, but most existing kernels between graphs are either purely local or purely global in character. In contrast, by building a hierarchy of nested subgraphs, the Multiscale Laplacian Graph kernels (MLG kernels) that we define in this paper can account for structure at a range of different scales. At the heart of the MLG construction is another new graph kernel, called the Feature Space Laplacian Graph kernel (FLG kernel), which has the property that it can lift a base kernel defined on the vertices of two graphs to a kernel between the graphs. The MLG kernel applies such FLG kernels to subgraphs recursively. To make the MLG kernel computationally feasible, we also introduce a randomized projection procedure, similar to the Nystr\"om method, but for RKHS operators.},
  date        = {2016-03-20},
  eprint      = {1603.06186v2},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1603.06186v2:PDF},
  keywords    = {stat.ML},
}

@InProceedings{Gori2005,
  author    = {M. Gori and G. Monfardini and F. Scarselli},
  title     = {A new model for learning in graph domains},
  booktitle = {Proceedings. 2005 {IEEE} International Joint Conference on Neural Networks, 2005.},
  year      = {2005},
  publisher = {{IEEE}},
  doi       = {10.1109/ijcnn.2005.1555942},
}

@Article{Micheli2009,
  author    = {A. Micheli},
  title     = {Neural Network for Graphs: A Contextual Constructive Approach},
  journal   = {{IEEE} Transactions on Neural Networks},
  year      = {2009},
  volume    = {20},
  number    = {3},
  pages     = {498--511},
  month     = {mar},
  doi       = {10.1109/tnn.2008.2010350},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Shuman2013,
  author    = {D. I. Shuman and S. K. Narang and P. Frossard and A. Ortega and P. Vandergheynst},
  title     = {The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains},
  journal   = {{IEEE} Signal Processing Magazine},
  year      = {2013},
  volume    = {30},
  number    = {3},
  pages     = {83--98},
  month     = {may},
  doi       = {10.1109/msp.2012.2235192},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{Zhang2018,
  author    = {Zhang, Muhan and Cui, Zhicheng and Neumann, Marion and Chen, Yixin},
  title     = {An end-to-end deep learning architecture for graph classification},
  booktitle = {Thirty-Second AAAI Conference on Artificial Intelligence},
  year      = {2018},
}

@Article{Lee2019,
  author      = {Junhyun Lee and Inyeop Lee and Jaewoo Kang},
  title       = {Self-Attention Graph Pooling},
  abstract    = {Advanced methods of applying deep learning to structured data such as graphs have been proposed in recent years. In particular, studies have focused on generalizing convolutional neural networks to graph data, which includes redefining the convolution and the downsampling (pooling) operations for graphs. The method of generalizing the convolution operation to graphs has been proven to improve performance and is widely used. However, the method of applying downsampling to graphs is still difficult to perform and has room for improvement. In this paper, we propose a graph pooling method based on self-attention. Self-attention using graph convolution allows our pooling method to consider both node features and graph topology. To ensure a fair comparison, the same training procedures and model architectures were used for the existing pooling methods and our method. The experimental results demonstrate that our method achieves superior graph classification performance on the benchmark datasets using a reasonable number of parameters.},
  date        = {2019-04-17},
  eprint      = {1904.08082v4},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1904.08082v4:PDF},
  keywords    = {cs.LG, stat.ML, I.2.6},
}

@Article{Weisfeiler1968,
  author  = {Weisfeiler, Boris and Lehman, Andrei A.},
  title   = {A reduction of a graph to a canonical form and an algebra arising during this reduction},
  journal = {Nauchno-Technicheskaya Informatsia},
  year    = {1968},
  volume  = {2},
  number  = {9},
  pages   = {12--16},
}

@Article{Bruna2013,
  author      = {Joan Bruna and Wojciech Zaremba and Arthur Szlam and Yann LeCun},
  title       = {Spectral Networks and Locally Connected Networks on Graphs},
  abstract    = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
  date        = {2013-12-21},
  eprint      = {1312.6203v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1312.6203v3:PDF},
  keywords    = {cs.LG, cs.CV, cs.NE},
}

@Article{Chen2018,
  author      = {Jie Chen and Tengfei Ma and Cao Xiao},
  title       = {FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling},
  abstract    = {The graph convolutional networks (GCN) recently proposed by Kipf and Welling are an effective graph model for semi-supervised learning. This model, however, was originally designed to be learned with the presence of both training and test data. Moreover, the recursive neighborhood expansion across layers poses time and memory challenges for training with large, dense graphs. To relax the requirement of simultaneous availability of test data, we interpret graph convolutions as integral transforms of embedding functions under probability measures. Such an interpretation allows for the use of Monte Carlo approaches to consistently estimate the integrals, which in turn leads to a batched training scheme as we propose in this work---FastGCN. Enhanced with importance sampling, FastGCN not only is efficient for training but also generalizes well for inference. We show a comprehensive set of experiments to demonstrate its effectiveness compared with GCN and related models. In particular, training is orders of magnitude more efficient while predictions remain comparably accurate.},
  date        = {2018-01-30},
  eprint      = {1801.10247v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1801.10247v1:PDF},
  keywords    = {cs.LG},
}

@Article{Chen2017,
  author      = {Jianfei Chen and Jun Zhu and Le Song},
  title       = {Stochastic Training of Graph Convolutional Networks with Variance Reduction},
  abstract    = {Graph convolutional networks (GCNs) are powerful deep neural networks for graph-structured data. However, GCN computes the representation of a node recursively from its neighbors, making the receptive field size grow exponentially with the number of layers. Previous attempts on reducing the receptive field size by subsampling neighbors do not have a convergence guarantee, and their receptive field size per node is still in the order of hundreds. In this paper, we develop control variate based algorithms which allow sampling an arbitrarily small neighbor size. Furthermore, we prove new theoretical guarantee for our algorithms to converge to a local optimum of GCN. Empirical results show that our algorithms enjoy a similar convergence with the exact algorithm using only two neighbors per node. The runtime of our algorithms on a large Reddit dataset is only one seventh of previous neighbor sampling algorithms.},
  date        = {2017-10-29},
  eprint      = {1710.10568v3},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1710.10568v3:PDF},
  keywords    = {stat.ML, cs.LG},
}

@Article{Chiang2019,
  author      = {Wei-Lin Chiang and Xuanqing Liu and Si Si and Yang Li and Samy Bengio and Cho-Jui Hsieh},
  title       = {Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks},
  abstract    = {Graph convolutional network (GCN) has been successfully applied to many graph-based applications; however, training a large-scale GCN remains challenging. Current SGD-based algorithms suffer from either a high computational cost that exponentially grows with number of GCN layers, or a large space requirement for keeping the entire graph and the embedding of each node in memory. In this paper, we propose Cluster-GCN, a novel GCN algorithm that is suitable for SGD-based training by exploiting the graph clustering structure. Cluster-GCN works as the following: at each step, it samples a block of nodes that associate with a dense subgraph identified by a graph clustering algorithm, and restricts the neighborhood search within this subgraph. This simple but effective strategy leads to significantly improved memory and computational efficiency while being able to achieve comparable test accuracy with previous algorithms. To test the scalability of our algorithm, we create a new Amazon2M data with 2 million nodes and 61 million edges which is more than 5 times larger than the previous largest publicly available dataset (Reddit). For training a 3-layer GCN on this data, Cluster-GCN is faster than the previous state-of-the-art VR-GCN (1523 seconds vs 1961 seconds) and using much less memory (2.2GB vs 11.2GB). Furthermore, for training 4 layer GCN on this data, our algorithm can finish in around 36 minutes while all the existing GCN training algorithms fail to train due to the out-of-memory issue. Furthermore, Cluster-GCN allows us to train much deeper GCN without much time and memory overhead, which leads to improved prediction accuracy---using a 5-layer Cluster-GCN, we achieve state-of-the-art test F1 score 99.36 on the PPI dataset, while the previous best result was 98.71 by [16]. Our codes are publicly available at https://github.com/google-research/google-research/tree/master/cluster_gcn.},
  date        = {2019-05-20},
  doi         = {10.1145/3292500.3330925},
  eprint      = {1905.07953v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1905.07953v2:PDF},
  keywords    = {cs.LG, cs.AI, stat.ML},
}

@Article{Hamilton2017,
  author      = {William L. Hamilton and Rex Ying and Jure Leskovec},
  title       = {Inductive Representation Learning on Large Graphs},
  abstract    = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
  date        = {2017-06-07},
  eprint      = {1706.02216v4},
  eprintclass = {cs.SI},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1706.02216v4:PDF},
  keywords    = {cs.SI, cs.LG, stat.ML},
}

@Article{Hartung2019,
  author    = {Hartung, Thomas},
  journal   = {EFSA Journal},
  title     = {Predicting toxicity of chemicals: software beats animal testing},
  year      = {2019},
  pages     = {e170710},
  volume    = {17},
  doi       = {10.2903/j.efsa.2019.e170710},
  groups    = {[clemens:]},
  publisher = {Wiley Online Library},
}

@Article{Luechtefeld2018,
  author    = {Thomas Luechtefeld and Dan Marsh and Craig Rowlands and Thomas Hartung},
  title     = {Machine Learning of Toxicological Big Data Enables Read-Across Structure Activity Relationships ({RASAR}) Outperforming Animal Test Reproducibility},
  journal   = {Toxicological Sciences},
  year      = {2018},
  volume    = {165},
  number    = {1},
  pages     = {198--212},
  month     = {jul},
  doi       = {10.1093/toxsci/kfy152},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Drwal2014,
  author    = {Malgorzata N. Drwal and Priyanka Banerjee and Mathias Dunkel and Martin R. Wettig and Robert Preissner},
  title     = {{ProTox}: a web server for the in silico prediction of rodent oral toxicity},
  journal   = {Nucleic Acids Research},
  year      = {2014},
  volume    = {42},
  number    = {W1},
  pages     = {W53--W58},
  month     = {may},
  doi       = {10.1093/nar/gku401},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Banerjee2018,
  author    = {Priyanka Banerjee and Andreas O. Eckert and Anna K. Schrey and Robert Preissner},
  title     = {{ProTox}-{II}: a webserver for the prediction of toxicity of chemicals},
  journal   = {Nucleic Acids Research},
  year      = {2018},
  volume    = {46},
  number    = {W1},
  pages     = {W257--W263},
  month     = {apr},
  doi       = {10.1093/nar/gky318},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Schmidt2009,
  author    = {U. Schmidt and S. Struck and B. Gruening and J. Hossbach and I. S. Jaeger and R. Parol and U. Lindequist and E. Teuscher and R. Preissner},
  title     = {{SuperToxic}: a comprehensive database of toxic compounds},
  journal   = {Nucleic Acids Research},
  year      = {2009},
  volume    = {37},
  number    = {Database},
  pages     = {D295--D299},
  month     = {jan},
  doi       = {10.1093/nar/gkn850},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Pope2018,
  author      = {Phillip Pope and Soheil Kolouri and Mohammad Rostrami and Charles Martin and Heiko Hoffmann},
  title       = {Discovering Molecular Functional Groups Using Graph Convolutional Neural Networks},
  abstract    = {Functional groups (FGs) are molecular substructures that are served as a foundation for analyzing and predicting chemical properties of molecules. Automatic discovery of FGs will impact various fields of research, including medicinal chemistry and material sciences, by reducing the amount of lab experiments required for discovery or synthesis of new molecules. In this paper, we investigate methods based on graph convolutional neural networks (GCNNs) for localizing FGs that contribute to specific chemical properties of interest. In our framework, molecules are modeled as undirected relational graphs with atoms as nodes and bonds as edges. Using this relational graph structure, we trained GCNNs in a supervised way on experimentally-validated molecular training sets to predict specific chemical properties, e.g., toxicity. Upon learning a GCNN, we analyzed its activation patterns to automatically identify FGs using four different explainability methods that we have developed: gradient-based saliency maps, Class Activation Mapping (CAM), gradient-weighted CAM (Grad-CAM), and Excitation Back-Propagation. Although these methods are originally derived for convolutional neural networks (CNNs), we adapt them to develop the corresponding suitable versions for GCNNs. We evaluated the contrastive power of these methods with respect to the specificity of the identified molecular substructures and their relevance for chemical functions. Grad-CAM had the highest contrastive power and generated qualitatively the best FGs. This work paves the way for automatic analysis and design of new molecules.},
  date        = {2018-12-01},
  eprint      = {1812.00265v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1812.00265v3:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Article{Du2017,
  author      = {Jian Du and Shanghang Zhang and Guanhang Wu and Jose M. F. Moura and Soummya Kar},
  title       = {Topology Adaptive Graph Convolutional Networks},
  abstract    = {Spectral graph convolutional neural networks (CNNs) require approximation to the convolution to alleviate the computational complexity, resulting in performance loss. This paper proposes the topology adaptive graph convolutional network (TAGCN), a novel graph convolutional network defined in the vertex domain. We provide a systematic way to design a set of fixed-size learnable filters to perform convolutions on graphs. The topologies of these filters are adaptive to the topology of the graph when they scan the graph to perform convolution. The TAGCN not only inherits the properties of convolutions in CNN for grid-structured data, but it is also consistent with convolution as defined in graph signal processing. Since no approximation to the convolution is needed, TAGCN exhibits better performance than existing spectral CNNs on a number of data sets and is also computationally simpler than other recent methods.},
  date        = {2017-10-28},
  eprint      = {1710.10370v5},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1710.10370v5:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Article{Li2018,
  author      = {Ruoyu Li and Sheng Wang and Feiyun Zhu and Junzhou Huang},
  title       = {Adaptive Graph Convolutional Neural Networks},
  abstract    = {Graph Convolutional Neural Networks (Graph CNNs) are generalizations of classical CNNs to handle graph data such as molecular data, point could and social networks. Current filters in graph CNNs are built for fixed and shared graph structure. However, for most real data, the graph structures varies in both size and connectivity. The paper proposes a generalized and flexible graph CNN taking data of arbitrary graph structure as input. In that way a task-driven adaptive graph is learned for each graph data while training. To efficiently learn the graph, a distance metric learning is proposed. Extensive experiments on nine graph-structured datasets have demonstrated the superior performance improvement on both convergence speed and predictive accuracy.},
  date        = {2018-01-10},
  eprint      = {1801.03226v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1801.03226v1:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Article{Gong2018,
  author      = {Liyu Gong and Qiang Cheng},
  title       = {Exploiting Edge Features in Graph Neural Networks},
  abstract    = {Edge features contain important information about graphs. However, current state-of-the-art neural network models designed for graph learning, e.g. graph convolutional networks (GCN) and graph attention networks (GAT), adequately utilize edge features, especially multi-dimensional edge features. In this paper, we build a new framework for a family of new graph neural network models that can more sufficiently exploit edge features, including those of undirected or multi-dimensional edges. The proposed framework can consolidate current graph neural network models; e.g. graph convolutional networks (GCN) and graph attention networks (GAT). The proposed framework and new models have the following novelties: First, we propose to use doubly stochastic normalization of graph edge features instead of the commonly used row or symmetric normalization approches used in current graph neural networks. Second, we construct new formulas for the operations in each individual layer so that they can handle multi-dimensional edge features. Third, for the proposed new framework, edge features are adaptive across network layers. As a result, our proposed new framework and new models can exploit a rich source of graph information. We apply our new models to graph node classification on several citation networks, whole graph classification, and regression on several molecular datasets. Compared with the current state-of-the-art methods, i.e. GCNs and GAT, our models obtain better performance, which testify to the importance of exploiting edge features in graph neural networks.},
  date        = {2018-09-07},
  eprint      = {1809.02709v2},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1809.02709v2:PDF},
  keywords    = {cs.LG, cs.SI, stat.ML},
}

@Article{Battaglia2018,
  author      = {Peter W. Battaglia and Jessica B. Hamrick and Victor Bapst and Alvaro Sanchez-Gonzalez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andrew Ballard and Justin Gilmer and George Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},
  title       = {Relational inductive biases, deep learning, and graph networks},
  abstract    = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  date        = {2018-06-04},
  eprint      = {1806.01261v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1806.01261v3:PDF},
  keywords    = {cs.LG, cs.AI, stat.ML},
}

@Article{Mobley2014,
  author    = {David L. Mobley and J. Peter Guthrie},
  title     = {{FreeSolv}: a database of experimental and calculated hydration free energies, with input files},
  journal   = {Journal of Computer-Aided Molecular Design},
  year      = {2014},
  volume    = {28},
  number    = {7},
  pages     = {711--720},
  month     = {jun},
  doi       = {10.1007/s10822-014-9747-x},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Wu2017,
  author      = {Zhenqin Wu and Bharath Ramsundar and Evan N. Feinberg and Joseph Gomes and Caleb Geniesse and Aneesh S. Pappu and Karl Leswing and Vijay Pande},
  title       = {MoleculeNet: A Benchmark for Molecular Machine Learning},
  abstract    = {Molecular machine learning has been maturing rapidly over the last few years. Improved methods and the presence of larger datasets have enabled machine learning algorithms to make increasingly accurate predictions about molecular properties. However, algorithmic progress has been limited due to the lack of a standard benchmark to compare the efficacy of proposed methods; most new algorithms are benchmarked on different datasets making it challenging to gauge the quality of proposed methods. This work introduces MoleculeNet, a large scale benchmark for molecular machine learning. MoleculeNet curates multiple public datasets, establishes metrics for evaluation, and offers high quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms (released as part of the DeepChem open source library). MoleculeNet benchmarks demonstrate that learnable representations are powerful tools for molecular machine learning and broadly offer the best performance. However, this result comes with caveats. Learnable representations still struggle to deal with complex tasks under data scarcity and highly imbalanced classification. For quantum mechanical and biophysical datasets, the use of physics-aware featurizations can be more important than choice of particular learning algorithm.},
  date        = {2017-03-02},
  eprint      = {1703.00564v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1703.00564v3:PDF},
  keywords    = {cs.LG, physics.chem-ph, stat.ML},
}

@InProceedings{Melnikov2019,
  author    = {Melnikov, Vitalik and H{\"u}llermeier, Eyke},
  title     = {Learning to Aggregate: Tackling the Aggregation/Disaggregation Problem for OWA},
  booktitle = {Proceedings of The Eleventh Asian Conference on Machine Learning},
  year      = {2019},
  editor    = {Lee, Wee Sun and Suzuki, Taiji},
  volume    = {101},
  series    = {Proceedings of Machine Learning Research},
  pages     = {1110--1125},
  address   = {Nagoya, Japan},
  month     = {17--19 Nov},
  publisher = {PMLR},
  abstract  = {The problem of “learning to aggregate” (LTA) has recently been introduced as a novel machine learning setting, in which instances are represented in the form of a composition of a (variable) number on constituents. Such compositions are associated with an evaluation, which is the target of the prediction task, and which can presumably be modeled in the form of a suitable aggregation of the properties of its constituents. An especially interesting class of LTA problems arises when the evaluations of the constituents are not available at training time, and instead ought to be learned simultaneously with the aggregation function. This scenario is referred to as the “aggregation/disaggregation problem”. In this paper, we tackle this problem for an interesting type of aggregation function, namely the Ordered Weighted Averaging (OWA) operator. In particular, we provide an algorithm for learning the OWA parameters together with local utility scores of the constituents, and evaluate this algorithm in a case study on predicting the performance of classifier ensembles.},
  file      = {melnikov19a.pdf:http\://proceedings.mlr.press/v101/melnikov19a/melnikov19a.pdf:PDF},
  url       = {http://proceedings.mlr.press/v101/melnikov19a.html},
}

@Article{Adamson1973,
  author    = {George W. Adamson and Judith A. Bush},
  title     = {A method for the automatic classification of chemical structures},
  journal   = {Information Storage and Retrieval},
  year      = {1973},
  volume    = {9},
  number    = {10},
  pages     = {561--568},
  month     = {oct},
  doi       = {10.1016/0020-0271(73)90059-4},
  publisher = {Elsevier {BV}},
}

@Article{Willett1986,
  author    = {Peter Willett and Vivienne Winterman},
  title     = {A Comparison of Some Measures for the Determination of Inter-Molecular Structural Similarity Measures of Inter-Molecular Structural Similarity},
  journal   = {Quantitative Structure-Activity Relationships},
  year      = {1986},
  volume    = {5},
  number    = {1},
  pages     = {18--25},
  doi       = {10.1002/qsar.19860050105},
  publisher = {Wiley},
}

@Online{TEST,
  title        = {{Toxicity Estimation Software Tool (TEST)}},
  organization = {EPA},
  url          = {https://www.epa.gov/chemical-research/toxicity-estimation-software-tool-test},
  urldate      = {2019-11-18},
}

@Online{ProTox,
  author       = {Banerjee, Priyanka and Preissner, Robert and Eckert, Andreas and Schrey, Anna K.},
  title        = {{ProTox-II - Prediction Of Toxicity Of Chemicals}},
  year         = {2018},
  organization = {{Charité – Universitätsmedizin Berlin}},
  url          = {http://tox.charite.de/protox_II/},
  urldate      = {2019-11-18},
}

@Online{ToxTrack,
  title        = {{ToxTrack} - Cheminformatics Modeling},
  organization = {ToxTrack Inc.},
  url          = {https://toxtrack.com/},
  urldate      = {2019-11-18},
}

@InProceedings{Le2014,
  author      = {Le, Quoc and Mikolov, Tomas},
  title       = {Distributed Representations of Sentences and Documents},
  booktitle   = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
  year        = {2014},
  series      = {ICML'14},
  pages       = {II-1188--II-1196},
  publisher   = {JMLR.org},
  acmid       = {3045025},
  eprint      = {1405.4053v2},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1405.4053v2:PDF},
  keywords    = {cs.CL, cs.AI, cs.LG},
  location    = {Beijing, China},
  url         = {http://dl.acm.org/citation.cfm?id=3044805.3045025},
}

@InProceedings{Mikolov2013,
  author      = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  title       = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle   = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
  year        = {2013},
  series      = {NIPS'13},
  pages       = {3111--3119},
  address     = {USA},
  publisher   = {Curran Associates Inc.},
  acmid       = {2999959},
  eprint      = {1310.4546v1},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/abs/1310.4546v1:PDF},
  keywords    = {cs.CL, cs.LG, stat.ML},
  location    = {Lake Tahoe, Nevada},
  numpages    = {9},
  url         = {http://dl.acm.org/citation.cfm?id=2999792.2999959},
}

@InProceedings{Perozzi2014,
  author      = {Bryan Perozzi and Rami Al-Rfou and Steven Skiena},
  title       = {{DeepWalk}: online learning of social representations},
  booktitle   = {Proceedings of the 20th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining - {KDD} {\textquotesingle}14},
  year        = {2014},
  publisher   = {{ACM} Press},
  doi         = {10.1145/2623330.2623732},
  eprint      = {1403.6652v2},
  eprintclass = {cs.SI},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1403.6652v2:PDF},
  keywords    = {cs.SI, cs.LG, H.2.8; I.2.6; I.5.1},
}

@InCollection{Immerman1990,
  author    = {Immerman, Neil and Lander, Eric},
  booktitle = {Complexity theory retrospective},
  publisher = {Springer},
  title     = {Describing graphs: A first-order approach to graph canonization},
  year      = {1990},
  pages     = {59--81},
  doi       = {10.1007/978-1-4612-4478-3_5},
}

@Article{Cai1992,
  author    = {Jin-Yi Cai and Martin Fürer and Neil Immerman},
  journal   = {Combinatorica},
  title     = {An optimal lower bound on the number of variables for graph identification},
  year      = {1992},
  month     = {dec},
  number    = {4},
  pages     = {389--410},
  volume    = {12},
  doi       = {10.1007/bf01305232},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Babai2015,
  author      = {László Babai},
  title       = {Graph Isomorphism in Quasipolynomial Time},
  abstract    = {We show that the Graph Isomorphism (GI) problem and the related problems of String Isomorphism (under group action) (SI) and Coset Intersection (CI) can be solved in quasipolynomial ($\exp((\log n)^{O(1)})$) time. The best previous bound for GI was $\exp(O(\sqrt{n\log n}))$, where $n$ is the number of vertices (Luks, 1983); for the other two problems, the bound was similar, $\exp(\tilde{O}(\sqrt{n}))$, where $n$ is the size of the permutation domain (Babai, 1983). The algorithm builds on Luks's SI framework and attacks the barrier configurations for Luks's algorithm by group theoretic "local certificates" and combinatorial canonical partitioning techniques. We show that in a well-defined sense, Johnson graphs are the only obstructions to effective canonical partitioning. Luks's barrier situation is characterized by a homomorphism {\phi} that maps a given permutation group $G$ onto $S_k$ or $A_k$, the symmetric or alternating group of degree $k$, where $k$ is not too small. We say that an element $x$ in the permutation domain on which $G$ acts is affected by {\phi} if the {\phi}-image of the stabilizer of $x$ does not contain $A_k$. The affected/unaffected dichotomy underlies the core "local certificates" routine and is the central divide-and-conquer tool of the algorithm.},
  date        = {2015-12-11},
  eprint      = {1512.03547v2},
  eprintclass = {cs.DS},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1512.03547v2:PDF},
  keywords    = {cs.DS, cs.CC, math.CO, math.GR, 68Q25, 68R10, 20B25, 20B15, 05E18, 05C65, 20L05, F.2.2; G.2.2},
}

@Article{Babai1980,
  author    = {Babai, László and Erdős, Paul and Selkow, Stanley M.},
  journal   = {SIaM Journal on computing},
  title     = {Random graph isomorphism},
  year      = {1980},
  number    = {3},
  pages     = {628--635},
  volume    = {9},
  publisher = {SIAM},
}

@Article{McKay2013,
  author      = {Brendan D. McKay and Adolfo Piperno},
  title       = {Practical graph isomorphism, II},
  abstract    = {We report the current state of the graph isomorphism problem from the practical point of view. After describing the general principles of the refinement-individualization paradigm and proving its validity, we explain how it is implemented in several of the key programs. In particular, we bring the description of the best known program nauty up to date and describe an innovative approach called Traces that outperforms the competitors for many difficult graph classes. Detailed comparisons against saucy, Bliss and conauto are presented.},
  date        = {2013-01-08},
  eprint      = {1301.1493v1},
  eprintclass = {cs.DM},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1301.1493v1:PDF},
  keywords    = {cs.DM, math.CO, 05C85, 68R10, 20B40},
}

@Online{McKay,
  author  = {McKay, Brendan and Piperno, Adolfo},
  title   = {{nauty and Traces}},
  url     = {http://pallini.di.uniroma1.it/index.html},
  urldate = {2020-02-21},
}

@Article{Newman2003,
  author    = {M. E. J. Newman},
  journal   = {{SIAM} Review},
  title     = {The Structure and Function of Complex Networks},
  year      = {2003},
  month     = {jan},
  number    = {2},
  pages     = {167--256},
  volume    = {45},
  doi       = {10.1137/s003614450342480},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
}

@Article{Welser2007,
  author  = {Welser, Howard T. and Gleave, Eric and Fisher, Danyel and Smith, Marc},
  title   = {Visualizing the signatures of social roles in online discussion groups},
  year    = {2007},
  doi     = {10.1.1.352.3618},
  journal = {Journal of Social Structure},
}

@Article{Kekule1866,
  author    = {Aug. Kekul{\'{e}}},
  journal   = {Annalen der Chemie und Pharmacie},
  title     = {Untersuchungen über aromatische Verbindungen. I. Ueber die Constitution der aromatischen Verbindungen.},
  year      = {1866},
  number    = {2},
  pages     = {129--196},
  volume    = {137},
  doi       = {10.1002/jlac.18661370202},
  publisher = {Wiley},
}

@InCollection{Fuerer2017,
  author    = {Martin Fürer},
  booktitle = {Lecture Notes in Computer Science},
  publisher = {Springer International Publishing},
  title     = {On the Combinatorial Power of the Weisfeiler-Lehman Algorithm},
  year      = {2017},
  pages     = {260--271},
  doi       = {10.1007/978-3-319-57586-5_22},
  eprint    = {1704.01023v1},
  eprintclass = {cs.DS},
  eprinttype  = {arXiv},
  file      = {:http\://arxiv.org/pdf/1704.01023v1:PDF},
}

@InProceedings{Kiefer2017,
  author     = {Sandra Kiefer and Ilia Ponomarenko and Pascal Schweitzer},
  booktitle  = {2017 32nd Annual {ACM}/{IEEE} Symposium on Logic in Computer Science ({LICS})},
  title      = {The Weisfeiler-Leman dimension of planar graphs is at most 3},
  year       = {2017},
  month      = {jun},
  publisher  = {{IEEE}},
  doi        = {10.1109/lics.2017.8005107},
  eprint     = {1708.07354v1},
  eprintclass = {cs.DM},
  eprinttype = {arXiv},
  file       = {:https\://arxiv.org/pdf/1708.07354:PDF},
}

@InCollection{Arvind2019,
  author     = {Vikraman Arvind and Frank Fuhlbrück and Johannes Köbler and Oleg Verbitsky},
  booktitle  = {Fundamentals of Computation Theory},
  publisher  = {Springer International Publishing},
  title      = {On Weisfeiler-Leman Invariance: Subgraph Counts and Related Graph Properties},
  year       = {2019},
  pages      = {111--125},
  doi        = {10.1007/978-3-030-25027-0_8},
  eprint     = {1811.04801v3},
  eprintclass = {cs.DM},
  eprinttype = {arXiv},
}

@Article{Alzaga2010,
  author     = {Afredo Alzaga and Rodrigo Iglesias and Ricardo Pignol},
  journal    = {Journal of Combinatorial Theory, Series B},
  title      = {Spectra of symmetric powers of graphs and the Weisfeiler{\textendash}Lehman refinements},
  year       = {2010},
  month      = {nov},
  number     = {6},
  pages      = {671--682},
  volume     = {100},
  doi        = {10.1016/j.jctb.2010.07.001},
  eprint     = {0801.2322v1},
  eprintclass = {math.SP},
  eprinttype = {arXiv},
  publisher  = {Elsevier {BV}},
}

@Article{Gu2015,
  author     = {Jiao Gu and Bobo Hua and Shiping Liu},
  journal    = {Discrete Applied Mathematics},
  title      = {Spectral distances on graphs},
  year       = {2015},
  month      = {aug},
  pages      = {56--74},
  volume     = {190-191},
  doi        = {10.1016/j.dam.2015.04.011},
  eprint     = {1402.6041v2},
  eprintclass = {math.SP},
  eprinttype = {arXiv},
  publisher  = {Elsevier {BV}},
}

@Article{Das2004,
  author    = {K.Ch. Das},
  journal   = {Computers {\&} Mathematics with Applications},
  title     = {The Laplacian spectrum of a graph},
  year      = {2004},
  month     = {sep},
  number    = {5-6},
  pages     = {715--724},
  volume    = {48},
  doi       = {10.1016/j.camwa.2004.05.005},
  publisher = {Elsevier {BV}},
}

@InProceedings{Borgwardt2005,
  author    = {K.M. Borgwardt and H. Kriegel},
  booktitle = {Fifth {IEEE} International Conference on Data Mining ({ICDM}{\textquotesingle}05)},
  title     = {Shortest-Path Kernels on Graphs},
  year      = {2005},
  publisher = {{IEEE}},
  doi       = {10.1109/icdm.2005.132},
}

@InProceedings{Morris2017,
  author      = {Christopher Morris and Kristian Kersting and Petra Mutzel},
  booktitle   = {2017 {IEEE} International Conference on Data Mining ({ICDM})},
  title       = {Glocalized Weisfeiler-Lehman Graph Kernels: Global-Local Feature Maps of Graphs},
  year        = {2017},
  month       = {nov},
  publisher   = {{IEEE}},
  doi         = {10.1109/icdm.2017.42},
  eprint      = {1703.02379v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
}

@Article{Milo2002,
  author    = {R. Milo},
  journal   = {Science},
  title     = {Network Motifs: Simple Building Blocks of Complex Networks},
  year      = {2002},
  month     = {oct},
  number    = {5594},
  pages     = {824--827},
  volume    = {298},
  doi       = {10.1126/science.298.5594.824},
  publisher = {American Association for the Advancement of Science ({AAAS})},
}

@Article{Chung2010,
  author  = {Chung, Fan},
  journal = {Notices of the American Mathematical Society},
  title   = {Graph theory in the information age},
  year    = {2010},
  month   = {06},
  volume  = {57},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Markings\;2\;1\;\;\;\;;
2 StaticGroup:[clemens:]\;2\;1\;\;\;\;;
}
