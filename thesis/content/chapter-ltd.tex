%!TEX root = ../main.tex
% chktex-file 46
\chapter{Learning to Decompose Graphs}%
\label{sec:ltd}

In the last chapter we saw how existing \ac{gcr} approaches relate to \ac{lta}.
Despite their differences, all described \ac{lta} formulations have one thing in common:
Their decomposition functions $\psi$ all split a given graph $G$ into constituents spanned by \ac{bfs} subtrees of $G$ or simply into connected components of $G$.
The only exception to this is the \ac{lta} formulation of \acp{svm} that use fingerprint embeddings; there the constituents are all isomorphic to handpicked substructures.

Apart from the fingerprint embedding approach, which requires domain knowledge in order to pick meaningful substructure patterns, current \ac{gcr} approaches use constituents that are at-best \textit{localized} but not necessarily \textit{interpretable}.
The defining characteristic of \ac{lta} proposed in \cref{sec:ltag:definition}, \textit{localized explainability}, is therefore only partially satisfied by existing approaches.
This shortcoming of the existing \ac{lta} formulations for structured input data gives rise to a new problem:
\begin{defn}
	The \ac{ltd} problem is solved by finding a graph decomposition function $\psi: \mathcal{G}_{\mathcal{D}} \to \mathcal{P}(\mathcal{G}_{\mathcal{D}})$ which splits all graphs $G \in \mathcal{G}_{\mathcal{D}}$ from a given domain $\mathcal{D}$ into constituent subgraphs which are individually ``meaningful'' in the domain $\mathcal{D}$.
\end{defn}

As per our definition of \ac{lta} from \cref{sec:ltag:definition}, the quality of an \ac{lta} formulation is determined by its chosen solution for the \ac{ltd} problem.
Since a comprehensive analysis of this problem would be beyond the scope of this thesis, we focus on the relation between \ac{ltd} and \acp{gcnn}.
The goal of this chapter is to answer the following two questions:
\begin{enumerate}
	\item \textit{Can a stack of graph convolution layers learn a decomposition function dynamically instead of using a static subtree decomposition?}
	\item \textit{What could be the foundation for such an ``\acs{ltd}-convolution'' layer?}
\end{enumerate}

We provide an answer to the first question in \cref{sec:ltd:edge-filter} by showing how decomposition functions $\psi$ can be learned via so-called \textit{edge filters} as part of a convolutional \ac{gnn} architecture.
This establishes the connection between the \acs{ltd} problem and graph convolutions on a high level.
Then the second question is tackled in \cref{sec:ltd:kgnn-limits,sec:ltd:wl2gnn}.
There a novel graph convolution approach is proposed which could serve as a starting point for a convolution layer which solves the \ac{ltd} problem.

\section{Learning Constituents via Edge Filters}%
\label{sec:ltd:edge-filter}

As we saw in \cref{sec:ltag:formulation:gcnn:conv}, the constituents of neighborhood convolutions are spanned by \ac{bfs} trees of depth $T$, where $T$ is the number of convolutional layers.
From this perspective the problem of learning constituents corresponds to learning a pruning operator on the branches of \ac{bfs} trees.
Such a pruning operator filters the edges that are traversed in each \ac{bfs} step.
There are two general edge filtering strategies:
\begin{enumerate}[label={\textbf{\arabic*.}}]
	\item \textbf{Edge prefiltering:}
		Here the edge filtering and convolution operations are performed in independent steps:
		First the edges of a given graph are filtered, then the convolution layers are applied to the filtered graph.
		This so-called \textit{edge prefiltering} strategy is illustrated in \cref{fig:ltd:subtree-filtering}.
		The main advantage of prefiltering is that it allows arbitrary combinations of edge filtering and convolution approaches.
		The main disadvantage is however that the same edges are removed in all \ac{bfs} subtrees.
		This restricts the expressive power of the learned decomposition function as shown in \cref{fig:ltd:dynamic-subtree-filtering}.
	\item \textbf{Dynamic edge filtering:}
		By filtering edges as part of the convolution operation itself, more flexible decompositions can be obtained.
		In the \textit{dynamic edge filtering} strategy the edge filter is part of the convolution operation and decides which neighbors of a given root node should be aggregated.
		Using this strategy a decomposition such as that shown in \cref{fig:ltd:dynamic-subtree-filtering} is learnable.
\end{enumerate}
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\linewidth]{gfx/ltd/subtree-filtering.pdf}
	\caption{Illustration of a pruned \ac{bfs} subtree when two edges are removed via prefiltering.}\label{fig:ltd:subtree-filtering}
\end{figure}
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.75\linewidth]{gfx/ltd/dynamic-subtree-filtering.pdf}
	\caption[A graph decomposition obtained via dynamic edge filtering which cannot be modeled by edge prefiltering.]{
		A graph decomposition obtained via dynamic edge filtering which cannot be modeled by edge prefiltering.
		Depending on the \ac{bfs} root node $v_{\mathrm{root}}$ different edges are removed.
		For $v_{\mathrm{root}} \in \textcolor{t_blue}{\{ v_3, v_4 \}}$ the edges $\{ (v_3, v_1), (v_3, v_2), (v_4, v_5), (v_4, v_6) \}$ are filtered out while for $v_{\mathrm{root}} \in \textcolor{t_red}{\{ v_1, v_2, v_5, v_6 \}}$ the edge $(v_3, v_4)$ is removed. % chktex 25
	}\label{fig:ltd:dynamic-subtree-filtering}
\end{figure}
Irrespective of the chosen edge filtering strategy, the decision whether to remove a given edge $e_{ij} = (v_i, v_j)$ or not has to be made based on relevant information about $e_{ij}$ and its surroundings.
One way to encode the information about $e_{ij}$ is via an edge feature vector $z_{ij} \in \mathbb{R}^d$ that is fed into the edge filter.

We propose that those edge feature vectors can be obtained via a 2-\acs{wl} inspired graph convolution layer since 2-\acs{wl} colors naturally represent the structural roles of edges as described in \cref{sec:related:character:wl}.
We already saw a \ac{gcnn} architecture in \cref{sec:related:gcr:nn} which could potentially compute such 2-\acs{wl} inspired edge feature vectors, the $k$-\acs{gnn}~\cite{Morris2019}.
Its variant for $k = 2$ produces edge feature vectors that can be used as the input for an edge filter.
However, as we will see in the next section, there are significant limitations to the discriminative and computational power of 2-\acsp{gnn}.

\section{Limitations of the Existing 2-\acs*{gnn}}%
\label{sec:ltd:kgnn-limits}

The $k$-\acs{gnn} is a \ac{gcnn} inspired by the $k$-\ac{wl} algorithm, it convolves feature vectors of vertex $k$-multisets.
In this section we will compare its $k = 2$ variant with the 2-\ac{wl} algorithm.
The main difference between the two boils down to their notion of ``neighborhood''.
As already briefly mentioned in \cref{sec:related:gcr:nn}, 2-\acsp{gnn} define the neighbors of an edge $\textcolor{t_blue}{e_{ij}} = (v_i, v_j)$ to be the edges that are incident to either $v_i$ or $v_j$. % chktex 25
In 2-\acs{wl} on the other hand, the neighbors of $\textcolor{t_blue}{e_{ij}}$ are the edge pairs ${\left\{(\textcolor{t_red}{e_{il}}, \textcolor{t_darkgreen}{e_{lj}})\right\}}_{v_l \in \mathcal{V}_G}$, i.e.\ all possible paths of length two that start at $v_i$ and end at $v_j$. % chktex 21 chktex 25
This difference becomes clear when comparing the definition of convolution in 2-\acsp{gnn} with that of color refinement in 2-\acs{wl} (see \cfullref[ on ]{eq:related:kgnn-layer} and \cfullref[ on ]{defn:related:wlk-refine}):
\begin{alignat*}{4}
	\text{2-\acs{gnn}\footnotemark: } && Z_G^{(t)}[e_{ij}] &= \sigma\Biggl( \textcolor{t_blue}{Z_G^{(t-1)}[e_{ij}]} W^{(t)} + &&\left( \smashoperator[r]{\sum_{v_l \in \Gamma_G(v_j)}} \textcolor{t_red}{Z_G^{(t-1)}[e_{il}]} + \smashoperator[lr]{\sum_{v_l \in \Gamma_G(v_i)}} \textcolor{t_darkgreen}{Z_G^{(t-1)}[e_{lj}]} \right) W_{\Gamma}^{(t)} \Biggr) \\ % chktex 25
	\text{2-\acs{wl}: } && \chi_{G,2}^{(t)}(e_{ij}) &= h\Bigl(\textcolor{t_blue}{\chi_{G,2}^{(t-1)}(e_{ij})}, &&\ldblbrace (\textcolor{t_red}{\chi_{G,2}^{(t-1)}(e_{il})}, \textcolor{t_darkgreen}{\chi_{G,2}^{(t-1)}(e_{lj})})\, |\, v_l \in \mathcal{V}_G \rdblbrace\Bigr) % chktex 25
\end{alignat*}\footnotetext{
	To highlight the relation between 2-\acsp{gnn} and 2-\acs{wl}, a 2-\acs{gnn} definition that is not generally correct is shown here;
	for self-loops with $i = j$ it incorrectly sums the feature vectors of neighboring edges twice.
	The correct general formula uses a single sum over $v_l \in {\Gamma_G(v_j) \cup \Gamma_G(v_i)}$.
}\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{gfx/ltd/2gnn-2wl-diff.pdf}
	\caption[Edge colorings on which 2-\acsp{gnn} and 2-\acs{wl} behave differently.]{
		Two edge colorings on which 2-\acsp{gnn} and 2-\acs{wl} behave differently.
		A 2-\acs{gnn} will refine the ``color vector'' of $e_{ij}$ to \textsf{\textbf{\small\textcolor{t_blue}{D}}} for both initial colorings.
		2-\acs{wl} on the other hand differentiates both colorings by preserving the color tuple information.
	}\label{fig:ltd:2gnn-2wl-diff}
\end{figure}

We will now analyze what those different notions of neighborhood imply for the discriminative and computational power of 2-\acsp{gnn} in comparison to 2-\acs{wl}.
In the first step we show that the discriminative power of 2-\acsp{gnn} on all graphs $G \in \mathcal{G}$ is upper bounded by that of 1-\acs{wl} on the so-called \textit{edge neighborhood graphs} $G^{\mathcal{E}} \in \mathcal{G}^{\mathcal{E}}$.
\begin{defn}\label{defn:ltd:edge-graph}
	The \textit{edge neighborhood graph} of a given graph $G = (\mathcal{V}_G, \mathcal{E}_G)$ is defined as $G^{\mathcal{E}} \coloneqq (\mathcal{V}_{G^{\mathcal{E}}}, \mathcal{E}_{G^{\mathcal{E}}})$ with the vertices $\mathcal{V}_{G^{\mathcal{E}}} \coloneqq \{ \ldblbrace v, u \rdblbrace\, |\, (v, u) \in \mathcal{E}_G \lor v = u \}$ and the edges $\mathcal{E}_{G^{\mathcal{E}}} \coloneqq \left\{ (e, e') \in \mathcal{V}_{\mathcal{G}^{\mathcal{E}}}^2 |\, \left|e \cap e'\right| = 1 \right\}$. % chktex 21
\end{defn}
\begin{prop}\label{prop:ltd:2gnn-wl1-limit}
	The discriminative power of all 2-\acsp{gnn} $h_2$ is bounded by that of 1-\ac{wl} on edge neighborhood graphs, i.e.\ $\forall\, G, H \in \mathcal{G}: G^{\mathcal{E}} \mathrel{\simeq_1} H^{\mathcal{E}} \rightarrow h_2(G) = h_2(H)$. % chktex 21
\end{prop}
\begin{proof}
	By \cref{defn:ltd:edge-graph} it holds that $\forall e_{ij} \in \mathcal{E}_G: {\Gamma_G(v_i) \cup \Gamma_G(v_j)} = \Gamma_{G^{\mathcal{E}}}(e_{ij})$.
	Therefore the 2-\acs{gnn} convolution operation defined in \cfullref[ on ]{eq:related:kgnn-layer} can be rewritten as a vertex neighborhood convolution operator
	\begin{align*}
		Z_G^{(t)}[e] = \sigma\left( Z_G^{(t-1)}[e] W^{(t)} + \smashoperator[lr]{\sum_{e' \in \Gamma_{G^{\mathcal{E}}}(e)}} Z_G^{(t-1)}[e'] W_{\Gamma}^{(t)} \right)
		\text{.}
	\end{align*}
	\Cref{prop:ltd:2gnn-wl1-limit} then follows from \cref{prop:related:gcnn-wl1-limit}~(\cpageref{prop:related:gcnn-wl1-limit}).
\end{proof}
\begin{lem}\label{lem:ltd:wl1-regular-edge-neighbor-limit}
	1-\ac{wl} cannot distinguish the edge neighborhood graphs $G^{\mathcal{E}}$ and $H^{\mathcal{E}}$ of any pair of $d$-regular graphs $G$ and $H$ with $n$ vertices.
\end{lem}
\begin{proof}
	Let $G$ and $H$ be two $d$-regular graphs of size $n$.
	Their corresponding edge neighborhood graphs $G^{\mathcal{E}}$ and $H^{\mathcal{E}}$ both have $n^{\mathcal{E}} = n + \frac{nd}{2}$ vertices.
	$n$ of those edge neighborhood vertices correspond to the vertices of $G$ and $H$ respectively, we will refer to them as \textit{loop vertices} $L_G$/$L_H$.
	The remaining $\frac{nd}{2}$ edge neighborhood vertices correspond to the edges of $G$ and $H$, we will refer to them as \textit{edge vertices} $E_G$/$E_H$.

	W.l.o.g.\ we define the initial colors of the loop vertices as $\chi^{(0)}(v) = \textcolor{t_blue}{\texttt{A}}$ for all $v \in L_G \cup L_H$. % chktex 25
	The initial colors of the edge vertices are defined as $\chi^{(0)}(e) = \textcolor{t_red}{\texttt{B}}$ for all $e \in E_G \cup E_H$. % chktex 25
	Note that each loop vertex $\ldblbrace v_i, v_i \rdblbrace$ with $v_i \in \mathcal{V}_G \cup \mathcal{V}_H$ has $d$ neighbors, the edges incident to $v_i$.
	Similarly each edge vertex $\ldblbrace v_i, v_j \rdblbrace$ has $2d$ neighbors, two of which are the loop vertices $\ldblbrace v_i, v_i \rdblbrace$ and $\ldblbrace v_j, v_j \rdblbrace$ with the remaining $2d - 2$ neighbors corresponding to the edges that are incident to $e_{ij}$.
	This is illustrated in \cref{fig:ltd:edge-neighborhood}.

	After one color refinement step we get $\chi^{(1)}(v) = h(\textcolor{t_blue}{\texttt{A}}, \ldblbrace \underbrace{\textcolor{t_red}{\texttt{B}}, \dots, \textcolor{t_red}{\texttt{B}}}_{d\text{ times}} \rdblbrace) \eqqcolon \textcolor{t_blue}{\texttt{C}}$ for all loop vertices $v \in  L_G \cup L_H$ and $\chi^{(1)}(e) = h(\textcolor{t_red}{\texttt{B}}, \ldblbrace \textcolor{t_blue}{\texttt{A}}, \textcolor{t_blue}{\texttt{A}}, \underbrace{\textcolor{t_red}{\texttt{B}}, \dots, \textcolor{t_red}{\texttt{B}}}_{\mathclap{2d - 2 \text{ times}}} \rdblbrace) \eqqcolon \textcolor{t_red}{\texttt{D}}$. % chktex 25
	This means that $\chi^{(0)}$ and $\chi^{(1)}$ are identical up to the color substitutions $\textcolor{t_blue}{\texttt{A}} \to \textcolor{t_blue}{\texttt{C}}$ and $\textcolor{t_red}{\texttt{B}} \to \textcolor{t_red}{\texttt{D}}$, i.e.\ $\chi^{(0)} \equiv \chi^{(1)}$, which in turn implies that 1-\ac{wl} terminates after one iteration. % chktex 25
	\Cref{lem:ltd:wl1-regular-edge-neighbor-limit} then directly follows since both $G^{\mathcal{E}}$ and $H^{\mathcal{E}}$ have $n$ vertices with the final color $\textcolor{t_blue}{\texttt{C}}$ and $\frac{nd}{2}$ vertices with the final color $\textcolor{t_red}{\texttt{D}}$, i.e.\ $G^{\mathcal{E}} \mathrel{\simeq_1} H^{\mathcal{E}}$. % chktex 25
\end{proof}
\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{gfx/ltd/edge-neighborhood.pdf}
	\caption{
		Illustration of the edge neighborhood graphs of two 2-regular graphs of size 6.
	}\label{fig:ltd:edge-neighborhood}
\end{figure}
\begin{prop}\label{prop:ltd:2gnn-regular-limit}
	A 2-\acs{gnn} cannot distinguish regular graphs of the same size and therefore has a lower discriminative power than 2-\ac{wl}.
\end{prop}
\begin{proof}
	The proposition directly follows from \cref{prop:ltd:2gnn-wl1-limit}, \cref{lem:ltd:wl1-regular-edge-neighbor-limit} and the fact that 2-\acs{wl} is able to distinguish most regular graphs~\cite[cor.~1.8.6]{Immerman1990}.
\end{proof}

As described in \cref{sec:related:character:wl} the discriminative power of a model is, by itself, not necessarily relevant for real-world graph datasets.
In \cfullref[ on ]{prop:related:wl2-cycle-count} we saw however that 2-\acs{wl} not only has a higher discriminative power than 1-\acs{wl} but also that it is able to count the number of $m$-cycles in a given graph for all $m \leq 7$.
This is relevant because cycle counts are a commonly used metric in real-world domains such as social network and molecular structure analysis~\cite{Milo2002}\cite{Newman2003}\cite{Welser2007}\cite{Adamson1973}\cite{Kekule1866}.
To conclude this section we now show that 2-\acsp{gnn} not only have a lower discriminative power than 2-\acs{wl} but are also unable to detect cycles.
\begin{prop}
	2-\acsp{gnn} cannot detect $m$-cycles for all $m \geq 3$.
\end{prop}
\begin{proof}
	Let $n$ be the \ac{lcm} of $3$ and some $m > 3$.
	We define $c_3 \coloneqq \frac{n}{3}$ and $c_m \coloneqq \frac{n}{m}$.
	Based on that we define the following two graphs:
	Let $G_3$ be a graph consisting of $c_3$ disconnected cycles of length $3$, analogously let $G_m$ be a graph consisting of $c_m$ disconnected cycles of length $m$.
	Since both $G_3$ and $G_m$ are 2-regular and have the size $n$, any 2-\ac{gnn} $h_2: \mathcal{G} \to \mathcal{Y}$ must map both of them to the same $y \in \mathcal{Y}$ by \cref{prop:ltd:2gnn-regular-limit}.

	Let us assume that $h_2$ is able to detect cycles of length $3$, i.e.\ triangles.
	By \cfullref[ on ]{defn:related:wl-count-detect} this would imply that there is a function $g: \mathcal{Y} \to \{ 0, 1\}$ s.t.\ $g(h_2(G_3)) = g(y) = 1$ and $g(h_2(G_m)) = g(y) = 0\ \lightning$.
	Conversely, assuming that $h_2$ is able to detect cycles of length $m > 3$, we obtain the contradiction $g(h_2(G_3)) = g(y) = 0$  and $g(h_2(G_m)) = g(y) = 1\ \lightning$.
	This concludes the proof.
\end{proof}

In this section we compared 2-\acsp{gnn} with the 2-\acs{wl} algorithm and found that they have a significantly lower discriminative and computational power than 2-\acs{wl}.
Motivated by those limitations we describe a novel graph convolution operator which is closer to 2-\acs{wl} in the following section.

\section{A Novel 2-\acs*{wl} Inspired \acs*{gnn}}%
\label{sec:ltd:wl2gnn}

We have seen that the main difference between 2-\acsp{gnn} and 2-\acs{wl} is their notion of ``neighborhood''.
In this section we describe a novel convolution operator which uses edge tuple neighbors just like 2-\acs{wl} to overcome the limitations of 2-\acsp{gnn}.
This will be done in three steps:
\begin{enumerate*}
	\item We begin by formally defining the 2-\acs{wl} convolution operator.
	\item Then the discriminative and computational power of this operator are analyzed.
	\item Lastly we describe how 2-\acs{wl} convolutions can be implemented efficiently on a modern \ac{gpgpu}.
\end{enumerate*}

\subsection{Definition of the 2-\acs*{wl} Convolution Operator}%
\label{sec:ltd:wl2gnn:definition}

Similar to the 2-LWL kernel described in \cref{sec:related:gcr:kernel}, our 2-\acs{wl} convolution operator reduces the computational cost of 2-\acs{wl} via two simplifications:
\begin{enumerate}[label=\textbf{\arabic*.}]
	\item \textbf{2-multisets:}
		Since we assume that graphs are undirected, $e_{ij}$ and $e_{ji}$ have identical feature vectors $x[e_{ij}] = x[e_{ji}] \in \mathcal{X}_{\mathcal{E}}$ and the same 2-\acs{wl} neighborhood.
		Instead of refining/convolving the feature vectors of 2-tuples $(v_i, v_j)$ we can therefore refine/convolve the feature vectors of 2-multisets $\ldblbrace v_i, v_j \rdblbrace$.
		This halves the number of feature vectors without affecting the discriminative or computational power of the 2-\acs{wl} convolution operator.
		To simplify the notation we assume that $e_{ij} = e_{ji} = \ldblbrace v_i, v_j \rdblbrace$ in the rest of this section.
	\item \textbf{Neighborhood localization:}
		Using the 2-multiset simplification, the original 2-\acs{wl} algorithm refines the color of all multisets $e_{ij} \in \mathcal{V}_G^2$ by hashing its current color and the colors of all neighbors ${\{ \ldblbrace e_{il}, e_{lj} \rdblbrace \}}_{v_l \in \mathcal{V}_G}$.
		This means that the time complexity of a single refinement step is $\mathcal{O}(n^3)$ for $n \coloneqq \left| \mathcal{V}_G \right|$ which quickly becomes infeasible for large graphs.

		To address this issue we reduce both the number of colored edges as well as the number of neighbors of each edge.
		This is achieved by only considering the edges that are part of the so-called \textit{$r$-th power of a given graph $G$} where $r \in \mathbb{N}$ is the freely choosable \textit{neighborhood radius}.
		\begin{defn}
			The \textit{$r$-th power of a graph $G$} is defined as
			\begin{align*}
				G^r \coloneqq \left(\mathcal{V}_G, \left\{ e_{ij} \in \mathcal{V}_G^2\, |\, d_{\mathrm{SP}, G}(v_i, v_j) \leq r \right\}\right) % chktex 21
			\end{align*}
			where $d_{\mathrm{SP}, G}(v_i, v_j)$ is the length of the shortest path between $v_i$ and $v_j$ in $G$.
			The distance of a vertex $v_i \in \mathcal{V}_G$ to itself is defined as $d_{\mathrm{SP}, G}(v_i, v_i) \coloneqq 0$.
			Note that $G^1$ does not generally equal $G$ because $G^1$ has self-loop edges $e_{ii} \in \mathcal{E}_{G^1}$ at all vertices.
		\end{defn}
		For the neighborhood radius $r = 1$ only the self-loop edges ${\{ e_{ii} \}}_{v_i \in \mathcal{V}_G}$ and the edges $\mathcal{E}_{G}$ are considered;
		for $r > 1$ edges between indirectly connected vertices are considered as well.
		\Cref{fig:ltd:neighborhood-radius} illustrates this for varying neighborhood radii.
		Through the reduction of the considered edges, the neighbors of each $e_{ij} \in \mathcal{E}_{G^r}$ are in turn reduced to the common $r$-neighbors of $v_i$ and $v_j$, i.e.\ $\{ \ldblbrace e_{il}, e_{lj} \rdblbrace\, |\, v_l \in {\Gamma_{G^r}(v_i) \cap \Gamma_{G^r}(v_j)} \}$. % chktex 21
		\begin{figure}[t]
			\centering
			\includegraphics[width=0.9\linewidth]{gfx/ltd/neighborhood-radius.pdf}
			\caption{
				Illustration of the powers of the six-cycle graph for varying $r$.
				The self-loop edge at each of the vertices is not explicitly shown.
				For $r = 3$ all possible edges between the six vertices will be considered just as in the original 2-\acs{wl} algorithm.
			}\label{fig:ltd:neighborhood-radius}
		\end{figure}

		Let us now consider what the reduced number of considered edges and the reduced number of edge neighbors implies for the runtime of a refinement step.
		If $G$ is a sparse graph with the maximum vertex degree $d \coloneqq \max_{v \in \mathcal{V}_G} \left|\Gamma_G(v)\right|$, the number of considered edges is bounded by $\mathcal{O}(n d^r)$ where each edge has at most $\mathcal{O}(d^r)$ neighbors.
		Consequently the time complexity of a refinement step becomes $\mathcal{O}(n d^{2r})$ which is a significant improvement over the $\mathcal{O}(n^3)$ bound of a full 2-\acs{wl} refinement step (assuming $d \ll n$).
\end{enumerate}
Based on the 2-multiset and the neighborhood localization simplifications we can now define the 2-\ac{wl} convolution operator.
\begin{defn}\label{defn:ltd:wl2-conv-init}
	The \textit{initial feature matrix $Z_G^{(0)}$} of the 2-\acs{wl} convolution operator with the neighborhood radius $r \in \mathbb{N}$ contains both the vertex features $x[v_i] \in \mathcal{X}_{\mathcal{V}} = \mathbb{R}^{d_{\mathcal{V}}}$ as well as the edge features $x[e_{ij}] \in \mathcal{X}_{\mathcal{E}} = \mathbb{R}^{d_{\mathcal{E}}}$ of a given graph $G$.
	More specifically $Z_G^{(0)} \in \mathbb{R}^{{|\mathcal{E}_{G^r}|} \times (d_{\mathcal{V}} + d_{\mathcal{E}})}$ assigns a row vector $Z_G^{(0)}[e_{ij}]$ to all edges $e_{ij} \in \mathcal{E}_{G^r}$.
	Those initial edge feature vectors are defined by the following vector concatenation:
	\begin{align*}
		Z_G^{(0)}[e_{ij}] \coloneqq \left( \begin{cases}
			x[v_i] & \text{if $i = j$} \\
			\mathbf{0} & \text{else}
		\end{cases} \right) \oplus \left( \begin{cases}
			x[e_{ij}] & \text{if $e_{ij} \in \mathcal{E}_G$} \\
			\mathbf{0} & \text{else}
		\end{cases} \right)
	\end{align*}
\end{defn}
\begin{defn}\label{defn:ltd:wl2-conv-step}
	We define the \textit{2-\acs{wl} graph convolution operator} as
	\begin{gather*}
		Z_G^{(t)}[e_{ij}] \coloneqq \sigma\left( Z_G^{(t-1)}[e_{ij}] W_{\mathrm{L}}^{(t)} + \smashoperator[lr]{\sum_{v_l \in {\Gamma_{G^r}(v_i) \cap \Gamma_{G^r}(v_j)}}} \kappa^{(t)}\left( Z_G^{(t-1)}[e_{ij}], \ldblbrace Z_G^{(t-1)}[e_{il}], Z_G^{(t-1)}[e_{lj}] \rdblbrace \right) \right) \\
		\text{with } \kappa^{(t)}(z_{ij}, \ldblbrace z_{il}, z_{lj} \rdblbrace) \coloneqq \left( z_{ij} W_{\mathrm{F}}^{(t)} \right) \odot \sigma_{\Gamma}\left(\left( z_{il} + z_{lj} \right) W_{\Gamma}^{(t)} \right)
		\text{.}
	\end{gather*}
\end{defn}
The convolution operator from \cref{defn:ltd:wl2-conv-step} is parameterized by the three matrices $W_{\mathrm{L}}^{(t)}, W_{\mathrm{F}}^{(t)},  W_{\Gamma}^{(t)} \in \mathbb{R}^{d^{(t-1)} \times d^{(t)}}$ and uses two freely choosable nonlinearities $\sigma$ and $\sigma_{\Gamma}$.
There are three properties which motivate this particular choice of convolution layer:
\begin{enumerate}[label=\textbf{\arabic*.}]
	\item \textbf{Simulation of \acp{mlp}:}
		By choosing $W_{F}^{(t)} = \mathbf{0}$ the convolution behaves like a regular fully connected layer.
		A stack of 2-\acs{wl} convolution layers can therefore simulate arbitrary \acp{mlp}.
	\item \textbf{Perservation of edge pair information:}
		The 2-\acs{wl} convolution layer computes a feature vector for each neighbor $\ldblbrace e_{il}, e_{lj} \rdblbrace$ of $e_{ij}$ via $\sigma_{\Gamma}\left(\left( z_{il} + z_{lj} \right) W_{\Gamma}^{(t)} \right)$.
		A 2-\acs{gnn} use a similar formulation but leaves out the inner nonlinearity $\sigma_{\Gamma}$;
		as we saw in \cref{fig:ltd:2gnn-2wl-diff} this causes 2-\acsp{gnn} to lose the edge pair information due to the commutativity and associativity of $+$.
	\item \textbf{Context-dependent neighborhood filtering:}
		Instead of filtering out all neighbors, $W_{F}^{(t)}$ also allows to filter the neighborhood of $e_{ij}$ more selectively.
		We can interpret $z_{ij} W_{\mathrm{F}}^{(t)} \in \mathbb{R}^{d^{(t)}}$ as a row vector of feature dimension weights.
		The feature dimensions of neighbors are rescaled via those weights which allows the model to aggregate different types of neighbors depending on $z_{ij}$.
\end{enumerate}

\subsection{Expressive Power of 2-\acs*{wl}-\acsp*{gnn}}%
\label{sec:ltd:wl2gnn:properties}

In this section we will analyze the discriminative and computational power of the 2-\ac{wl} convolution operator that was just defined.
We begin by showing that 2-\acs{wl}-\acsp{gnn} are at least as powerful as 1-\acs{wl}.

\begin{thm}
	A stack of 2-\acs{wl} convolution layers can simulate a stack of \ac{gin} convolution layers as defined in \cfullref[ on ]{eq:related:gin-layer}, i.e.\ all vertices $v_i \in \mathcal{V}_G$ of a given graph $G$ have the same feature vectors after applying the \ac{gin} convolutions as the self-loops $e_{ii}$ after applying the 2-\acs{wl} convolutions.
\end{thm}
\begin{proof}
	A stack of \ac{gin} convolution layers can be described as a sequence of neighborhood aggregations of the form $Z_G^{(t)}[v_i] = (1 + \varepsilon) Z_G^{(t-1)}[v_i] + \sum_{v_j \in \Gamma_G(v_i)} Z_G^{(t-1)}[v_j]$ with fully connected layers interspersed between them\footnote{
		The $(1 + \varepsilon) \in \mathbb{R}$ factor is required to guarantee the injectivity and therefore 1-\acs{wl} equivalence of \acp{gin}~\cite{Xu2018}.
		In our previous description of \ac{gin} we assumed that $\varepsilon = 0$ for simplicity.
		However for the formal correctness of this proof we also have to account for the case of $\varepsilon > 0$.
	}.
	As already mentioned, the 2-\acs{wl} convolution layer is reduced to a fully connected layer if $W_{F}^{(t)} = \mathbf{0}$.
	To show that 2-\acs{wl} convolutions can simulate \ac{gin}, it is therefore sufficient to show that for the $t$-th \ac{gin} layer there is a stack $S^{(t)}$ of 2-\ac{wl} convolution layers s.t.\ %
	\begin{align*}
		\exists P^{(t)}: \forall v_i \in \mathcal{V}_G: S^{(t)}\left(Z_{G}^{(t-1)}\right)[e_{ii}] P^{(t)} = (1 + \varepsilon) Z_{G}^{(t-1)}[e_{ii}] P^{(t)} + \smashoperator[lr]{\sum_{v_l \in \Gamma_G(v_i)}} Z_{G}^{(t-1)}[e_{ll}] P^{(t)}
		\text{.}
	\end{align*}
	Because \ac{gin}'s neighborhood aggregation is described by a sum of vertex feature vectors, the individual vertex feature dimensions can be treated independently.
	W.l.o.g.\ we will therefore assume that each vertex only has a single scalar feature to simplify notation.

	We now inductively construct the set of 2-\acs{wl} stacks ${\{ S^{(t)} \}}_{t=1}^T$. % chktex 21
	A neighborhood radius of $r = 1$ will be sufficient to simulate the \ac{gin} aggregation.
Therefore each convolved feature matrix $Z_G^{(t)}$ contains $m \coloneqq \left|\mathcal{E}_{G^1}\right| = \left|\mathcal{V}_G\right| + \left|\mathcal{E}_G\right|$ rows, one for each vertex/self-loop and one for each edge of the input graph $G$.
	Based on the assumption that each vertex has one-dimensional features, the initial feature matrix $Z_G^{(0)}$ can be described by the following row vectors:
	\begin{align*}
		\forall v_i \in \mathcal{V}_G: Z_G^{(0)}[e_{ii}] \coloneqq (1, x[v_i], 0)
		\quad\text{and}\quad
		\forall e_{ij} \in \mathcal{E}_G: Z_G^{(0)}[e_{ij}] \coloneqq (0, 0, 1)
		\text{.}
	\end{align*}
	The first stack $S^{(1)} \coloneqq S^{(1, 2)} \circ S^{(1, 1)}$ takes those initial feature vector and performs a neighborhood aggregation by applying two 2-\acs{wl} convolution layers.
	The first 2-\acs{wl} layer $S^{(1, 1)}: \mathbb{R}^{m \times 3} \to \mathbb{R}^{m \times 4}$ is defined as
	\begin{gather*}
		\begin{aligned}
			S^{(1, 1)}\left(Z^{(0)}\right)[e_{ij}] &= \textcolor{t_blue}{Z^{(0)}[e_{ij}] W_L^{(1,1)}} + \smashoperator[lr]{\sum_{v_l \in {\Gamma_{G^1}(v_i) \cap \Gamma_{G^1}(v_j)}}} \textcolor{t_red}{\left( Z^{(0)}[e_{ij}] W_F^{(1,1)}\right)} \odot \textcolor{t_darkgreen}{\left( \left( Z^{(0)}[e_{il}] + Z^{(0)}[e_{lj}] \right) W_{\Gamma}^{(1,1)} \right)} \\ % chktex 25
			&= \begin{cases}
				\textcolor{t_blue}{(1, \varepsilon x[v_i], 0, 0)} - \left|\Gamma_{G^1}(v_i)\right| \textcolor{t_red}{(0, x[v_i], 0, 0)} & \text{if $i = j$} \\ % chktex 25
				\textcolor{t_blue}{(0, 0, 1, 0)} + \textcolor{t_darkgreen}{(0, 0, 0, x[v_i])} + \textcolor{t_darkgreen}{(0, 0, 0, x[v_j])} & \text{else} % chktex 25
			\end{cases} \\
			&= \begin{cases}
				\left(1, (\varepsilon - \left|\Gamma_{G^1}(v_i)\right|) x[v_i], 0, 0\right) & \text{if $i = j$} \\
				(0, 0, 1, x[v_i] + x[v_j]) & \text{else}
			\end{cases}
		\end{aligned}\\
		\text{with }
		W_L^{(1,1)} \coloneqq \begin{pmatrix}
			1 & 0 & 0 & 0 \\
			0 & \varepsilon & 0 & 0 \\
			0 & 0 & 1 & 0
		\end{pmatrix},
		W_F^{(1,1)} \coloneqq \begin{pmatrix}
			0 & 0 & 0 & 0 \\
			0 & -\frac{1}{2} & 0 & 0 \\
			0 & 0 & 0 & 1
		\end{pmatrix},
		W_{\Gamma}^{(1,1)} \coloneqq \begin{pmatrix}
			0 & 1 & 0 & 0 \\
			0 & 0 & 0 & 1 \\
			0 & 1 & 0 & 0
		\end{pmatrix}
		\text{.}
	\end{gather*}
	The aggregation is then completed in the second layer $S^{(1, 2)}: \mathbb{R}^{m \times 4} \to \mathbb{R}^{m \times 3}$:
	\begin{gather*}
		\begin{aligned}
			S^{(1)}\left(Z^{(0)}\right)[e_{ij}] &= \begin{cases}
				\textcolor{t_blue}{\left(1, (\varepsilon - \left|\Gamma_{G^1}(v_i)\right|) x[v_i], 0\right)} + \displaystyle\smashoperator[lr]{\sum_{v_l \in \Gamma_{G^1}(v_i)}} \textcolor{t_darkgreen}{(0, x[v_i] + x[v_l], 0)} & \text{if $i = j$} \\ % chktex 25
				\textcolor{t_blue}{(0, 0, 1)} & \text{else} % chktex 25
			\end{cases} \\
			&= \begin{cases}
				\left(1, (1 + \varepsilon) x[v_i] + \displaystyle\smashoperator[lr]{\sum_{v_l \in \Gamma_{G}(v_i)}} x[v_l], 0\right) & \text{if $i = j$} \\
				(0, 0, 1) & \text{else}
			\end{cases}
		\end{aligned}\\
		\text{with }
		W_L^{(1,2)} \coloneqq \begin{pmatrix}
			1 & 0 & 0 \\
			0 & 1 & 0 \\
			0 & 0 & 1 \\
			0 & 0 & 0
		\end{pmatrix},\quad
		W_F^{(1,2)} \coloneqq \begin{pmatrix}
			0 & \frac{1}{2} & 0 \\
			0 & 0 & 0 \\
			0 & 0 & 0 \\
			0 & 0 & 0
		\end{pmatrix},\quad
		W_{\Gamma}^{(1,2)} \coloneqq \begin{pmatrix}
			0 & 0 & 0 \\
			0 & 0 & 0 \\
			0 & 0 & 0 \\
			0 & 1 & 0
		\end{pmatrix}
		\text{.}
	\end{gather*}
\end{proof}

\subsection{Implementation of 2-\acs*{wl}-\acsp*{gnn} on \acsp*{gpgpu}}%
\label{sec:ltd:wl2gnn:implementation}
