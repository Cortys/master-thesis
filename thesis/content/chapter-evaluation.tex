%!TEX root = ../main.tex
% chktex-file 46
\chapter{Evaluation}%
\label{sec:eval}

In \cref{sec:ltag} the relation between \ac{lta} and existing \ac{gcr} approaches was formally analyzed.
There we saw that the \ac{lta} formulations of existing approaches mostly use static decomposition functions, e.g.\ \ac{bfs} subtree decompositions.
Motivated by the idea of dynamically learning decompositions via edge filters, we then proposed the novel 2-\acs{wl}-\acs{gnn} in \cref{sec:ltd}.
The ides presented in both chapters will now be empirically evaluated.
Do to so we differentiate between two mostly independent evaluation aspects:
\begin{enumerate}[label={\textbf{\arabic*.}}]
	\item \textbf{Evaluation of 2-\acs{wl}-\acsp{gnn}:}
		Even though it was motivated by \ac{lta}, a 2-\acs{wl}-\acs{gnn} is not generally more ``\acs{lta}-like'' than other approaches.
		Nonetheless, due to the theoretical advantages described in \cref{sec:ltd:wl2gnn:properties} it is an interesting approach independently from its potential applications in \ac{lta} (see \cref{sec:ltd:edge-filter}).
		Thus the first aspect of our evaluation is to compare 2-\acs{wl}-\acsp{gnn} with the other previously described \ac{gcr} methods in a general non-\acs{lta} fashion, i.e.\ with an added \ac{mlp} after the pooling layer since this is how \acp{gnn} are typically evaluated in other works.
	\item \textbf{Evaluation of the \ac{lta} assumption:}
		We previously described that a given domain problem satisfies the \ac{lta} assumption if its solutions can be described by an \ac{lta} formulation (see \cfullref{defn:ltag:lta-assumption}).
		The inherent bias of an \acs{lta}-like model towards such \ac{lta} formulations could potentially increase its generalization performance compared to more general non-\acs{lta} models.
		Therefore the second aspect of our evaluation is to compare the performance of the previously described \acs{lta}-like methods with that of non-\acs{lta} approaches on datasets from multiple problem domains.
\end{enumerate}
This chapter will tackle those two aspects in three steps:
\begin{enumerate*}[label={\circled{\small\arabic*}}]
	\item We begin by describing the experimental setup used to obtain the evaluation results in \cref{sec:eval:setup}.
	\item We then present results on synthetically generated data in \cref{sec:eval:synthetic}.
	 	There we will illustrate the higher expressive power of 2-\acs{wl}-\acsp{gnn} when compared to other \ac{gcr} approaches which confirms the theoretical results from \cref{sec:ltd:wl2gnn:properties}.
	\item Finally evaluation results on real-world datasets are described in \cref{sec:eval:real}.
		There we will see how 2-\acs{wl}-\acsp{gnn} compare to other \acp{gnn} in practice as well as how \acs{lta}-like models compare to non-\acs{lta} models.
\end{enumerate*}

\section{Experimental Setup}%
\label{sec:eval:setup}

In our experimental evaluation we focus on two types of learners:
\acp{svm} using graph kernels and \acp{gcnn}.
We evaluate those learners by comparing their test accuracies on multiple binary classification problems.
To obtain those accuracies we follow the graph classification benchmarking framework recently proposed by \citet{Errica2020}.
Their benchmarking framework is motivated by the observation that most recent publications in the field of \acp{gnn} do not provide reproducible results.
To tackle this issue they evaluated multiple state-of-the-art methods using a unified model selection procedure:\\
{\setlength{\intextsep}{0pt}
\begin{minipage}[t]{0.55\linewidth-1em}
	\begin{algorithm}[H]
		\caption{$k$-fold Model Assessment}\label{algo:eval:assessment}
		\begin{algorithmic}[1]
			\State{\textbf{Input:} Dataset $\mathcal{D}$, configurations $\Theta$}
			\State{Split $\mathcal{D}$ into $k$ folds $F_1, \dots, F_{k}$}
			\For{$i \leftarrow 1, \dots, k$}
				\State{$\mathcal{D}_{\mathrm{train/val}}, \mathcal{D}_{\mathrm{test}} \leftarrow \left( \bigcup_{j \neq i} F_{j} \right), F_i$}
				\State{Split $\mathcal{D}_{\mathrm{train/val}}$ into $\mathcal{D}_{\mathrm{train}}, \mathcal{D}_{\mathrm{val}}$}
				\State{$\theta_{\mathrm{best}} \leftarrow \Call{Select}{\mathcal{D}_{\mathrm{train}}, \mathcal{D}_{\mathrm{val}}, \Theta}$}
				\For{$r \leftarrow 1, \dots, R$}
					\State{$h_{i,r} \leftarrow \Call{Train}{\mathcal{D}_{\mathrm{train}}, \theta_{\mathrm{best}}}$}
					\State{$\mathit{acc}_{i,r} \leftarrow \Call{Eval}{h_{i,r}, \mathcal{D}_{\mathrm{test}}}$}
				\EndFor{}
				\State{$\mathit{acc}_{i} \leftarrow \mean_{r \in [R]}{\mathit{acc}_{i,r}}$}
			\EndFor{}
			\State{\Return{$\mean_{i \in [k]} \mathit{acc}_i, \mathrm{stddev}_{i \in [k]}\, \mathit{acc}_i$}}
		\end{algorithmic}
	\end{algorithm}
\end{minipage}\hspace*{1em}%
\begin{minipage}[t]{0.45\linewidth}
	\begin{algorithm}[H]
		\caption{Model Selection}\label{algo:eval:selection}
		\begin{algorithmic}[1]
			\Function{Select}{$\mathcal{D}_{\mathrm{train}}, \mathcal{D}_{\mathrm{val}}, \Theta$}
			\ForAll{$\theta \in \Theta$}
				\State{$h_{\theta} \leftarrow \Call{Train}{\mathcal{D}_{\mathrm{train}}, \theta}$}
				\State{$\mathit{acc}_{\theta} \leftarrow \Call{Eval}{h_{\theta}, \mathcal{D}_{\mathrm{val}}}$}
			\EndFor{}
			\State{$\theta_{\mathrm{best}} \leftarrow \arg\max_{\theta \in \Theta}{\mathit{acc}_{\theta}}$}
			\State{\Return{$\theta_{\mathrm{best}}$}}
			\EndFunction{}
		\end{algorithmic}
	\end{algorithm}
\end{minipage}}

We base our evaluations on this assessment strategy with $k = 10$ folds and $r = 3$ repeats per fold to smooth out differences caused by random weight initializations.
For each dataset the same folds are used across the evaluated models; class proportions are preserved within each fold by using stratified splits.
To keep the total runtime of the experiments feasible, a single 90\%/10\% holdout split into training and validation data is used instead of cross-validation.
In each experiment, training is performed with an early stopping condition which cancels the optimization if there is no improvement to the validation loss for more than $p$ epochs.
The patience period $p$ is part of the hyperparameter configurations $\theta \in \Theta$.

Using this assessment strategy we evaluate \acp{svm} with the following graph kernels:
\begin{enumerate}[label={\textbf{\arabic*.}},itemsep=2pt,parsep=2pt]
	\item \textbf{\ac{wl} subtree kernel (\acs{wl}\textsubscript{ST})} with the iteration counts $T \in [5]$ to evaluate the influence of the depth of \ac{bfs} subtrees which span \ac{lta} constituents.
	\item \textbf{\ac{wl} shortest path kernel (\acs{wl}\textsubscript{SP})} with the iteration count $T = 5$.
	\item \textbf{2-LWL kernel} with the iteration count $T = 3$.
	\item \textbf{2-GWL kernel} with the iteration count $T = 3$.
\end{enumerate}
The gram matrices of the \acs{wl}\textsubscript{ST} and \acs{wl}\textsubscript{SP} kernels are computed via the \citetitle{GK} library~\cite{Siglidis2018}\cite{GK}.
For the gram matrices of the two dimensional \ac{wl} kernels we use a modified version\footnote{\url{https://github.com/Cortys/glocalwl}} of the reference implementation provided by \citet{Morris2017}.
To train \acp{svm} with those kernels, \citetitle{SKL}~\cite{Pedregosa2011}\cite{SKL} is used.
For the evaluation of \acp{gcnn} we selected the following methods:
\begin{enumerate}[label={\textbf{\arabic*.}},itemsep=2pt,parsep=2pt]
	\item \textbf{Structure unaware baseline:}
		\citet{Errica2020} describe a simple model which simply applies a standard \ac{mlp} to each individual vertex feature vector, then sums up the resulting feature vectors and applies another \ac{mlp} to the vector sum.
		This approach does not use any structural information and therefore serves as a baseline to detect whether a \ac{gnn} is able to exploit graph structure.
	\item \textbf{\ac{gin}} is evaluated as described by \citet{Xu2018}, i.e.\ with a sum pooling layer and an appended \ac{mlp} to produce the final prediction.
	\item \textbf{2-\acs{gnn}} is evaluated with both a static $\mean$ pooling layer and with \ac{sampool} (see \cfullref{defn:ltag:sam-pool}).
		After the pooling layer a \ac{mlp} is used to produce the final prediction.
	\item \textbf{2-\acs{wl}-\ac{gnn}} \textit{(our method)} is evaluated using the same configurations as 2-\acs{gnn}.
		To test the \ac{lta} assumption we additionally evaluate it in \acs{lta}-like configurations, i.e.\ with a stack of 2-\acs{wl} convolutions that produce a local prediction $y_{ij} \in [0, 1]$ for each edge $e_{ij}$ and without the final \ac{mlp}.
\end{enumerate}
The baseline and \ac{gin} results are obtained using the PyTorch-based implementation provided by \citet{Errica2020}.
For both 2-\acs{gnn} and 2-\acs{wl}-\ac{gnn} a custom TensorFlow-based implementation is used.
The code for all conducted experiments as well as the used dataset splits are available GitHub\footnote{\url{https://github.com/Cortys/master-thesis}}.

\section{Evaluation on Synthetic Data}%
\label{sec:eval:synthetic}

We begin with an evaluation of graph kernels and \acp{gnn} on a synthetic binary classification dataset which demonstrates the potential advantages of a higher dimensional \ac{wl} method such as the proposed 2-\acs{wl}-\acs{gnn}.
To determine the classes of the graphs in this dataset, a learner has to solve the so-called \textit{unicolored triangle detection} problem:
Given a graph $G$ with vertices that are colored as either $l_G[v] = \colorlabel{t_blue}{A}$ or as $l_G[v] = \colorlabel{t_red}{B}$, the learner has to find the unique triangle $(v_i, v_j, v_k)$ in $G$ for which $l_G[v_i] = l_G[v_j] = l_G[v_k]$. % chktex 25
The class of $G$ is then determined by the color of the vertices $(v_i, v_j, v_k)$.

Based on this problem we generated a synthetic triangle detection dataset.
It contains randomly generated graphs with varying vertex counts and vertex color proportions (see \cref{sec:appendix:ds-stats} for a more detailed description).
We use this dataset to evaluate whether a learner is able to ignore varying amounts of noisy random structure and focus on relevant local substructures, in this case unicolored triangles.
\begin{table}[t]
	\caption{Mean accuracies and standard deviations on the triangle detection dataset.}\label{tbl:eval:synthetic}
	\centering
	\csvreader[
		tabular={clrrr},
		separator=semicolon,
		table head={%
			& \multicolumn{2}{l}{Model (Iterations/Pooling)} & \multicolumn{1}{c}{Train} & \multicolumn{1}{c}{Test} \\\toprule%
		},
		table foot=\bottomrule,
		late after line=\ifthenelse{\equal{\id}{9}}{\\\midrule}{\\},
		head to column names,
		filter=\equal{\isDefault}{1}\or\equal{\id}{0}
	]{data/results.csv}{}{%
		\ifthenelse{\equal{\id}{0}}{\multirow{5}{*}[0em]{\rotatebox[origin=c]{90}{\small\textsc{Kernel}}}}{}%
		\ifthenelse{\equal{\id}{9}}{\multirow{8}{*}[0em]{\rotatebox[origin=c]{90}{\small\textsc{\ac{gnn}}}}}{} &%
		\textbf{\model} ($\params$) & \ifthenelse{\equal{\isLta}{1}}{\badgeboxinline[t_green]{\scriptsize \acs*{lta}-like}}{} &%
		$\condbold{\triangleBestTrain}{\triangleTrainMean} \pm \triangleTrainStd$ & $\condbold{\triangleBestTest}{\triangleTestMean} \pm \triangleTestStd$%
	}
\end{table}
\begin{figure}[t]
	\centering
	\includegraphics[width=0.7\linewidth]{gfx/evaluation/triangle-problem.pdf}
	\caption[Two example graphs from the triangle detection dataset.]{
		Two examples from the triangle detection dataset.
		The unique unicolored triangle which has to be detected by the learners is highlighted in both graphs.
	}\label{fig:evaluation:triangle-problem}
\end{figure}
Looking at the results in \cref{tbl:eval:synthetic}, it can be seen that the structure unaware baseline method is completely unable to do so, as expected.
The structure aware learners on the other hand all perform better than random guessing and are in fact mostly able to fit the training data perfectly.
This shows that all generated graphs are 1-\acs{wl} distinguishable; the \ac{wl} subtree kernel for example can simply memorize the training graphs via their unique 1-\acs{wl} color distribution after $T = 5$ refinement steps.

However, the ability to distinguish training graphs is not sufficient to also classify previously unseen graphs correctly.
Since 1-\acs{wl} cannot detect triangles, all 1-\acs{wl} bounded approaches (\acs{wl}\textsubscript{ST}, \acs{wl}\textsubscript{SP}, Baseline, \acs{gin}) are therefore unable to generalize, as can be seen in their test accuracies.
The fact that they perform better than random guessing can be explained by the following proxy indicator:
The presence of an \colorlabel{t_blue}{A}-colored triangle in a graph $G$ implies that there is a local region with a slightly higher density of \colorlabel{t_blue}{A}-colored vertices than in a \colorlabel{t_red}{B}-colored graph $H$ with the same vertex color proportions.
This local color density difference is already detectable in the depth-1 \acs{bfs} subtrees used by 1-\acs{wl} after a single refinement step; this explains why \acs{wl}\textsubscript{ST} performs similarly for $T = 1$ and $T = 5$.

Let us now take a look at the 2-\acs{wl} inspired kernels: 2-LWL and 2-GWL.\@
Interestingly both kernels do not appear to generalize better the 1-\acs{wl} bounded methods;
we explain this by the fairly small size of the triangle detection dataset (228 graphs).
Even though both kernels embed graphs into a space with dimensions that indicate the presence of a unicolored triangle (see \cfullref{prop:related:wl2-cycle-count}), there are many of those triangle-indicating embedding dimensions s.t.\ the indicating dimensions found in a given training split might not overlap with those in the test split.

Looking at the 2-\acs{wl} inspired \acp{gnn} (2-\acs{gnn}, 2-\acs{wl}-\acs{gnn}), we find that the proposed 2-\acs{wl}-\acs{gnn} significantly outperforms all other evaluated methods which confirms our theoretical results from \cref{sec:ltd:wl2gnn:properties}.
We evaluated 2-\acs{wl}-\acsp{gnn} in an \acs{lta}-like configuration without a final \ac{mlp} and in a non-\acs{lta} configuration with an \ac{mlp} after the pooling layer.
Comparing both configurations we clearly see that the \acs{lta}-variant has a worse generalization performance despite the fact that the triangle detection problem fits well into the \ac{lta} framework:
Find the unicolored triangle constituent, classify it locally then use that local class to classify the graph.
Further investigations are required to determine to which extent the worse performance of the \acs{lta}-like configuration is caused by the static subtree-based constituents and whether a more dynamic solution to the \acf{ltd} problem could improve the performance (e.g.\ via the edge filters described in \cref{sec:ltd:edge-filter}).

Finally, looking at the two evaluated \acs{lta}-compatible pooling layers, static $\mean$ pooling and \ac{sampool}, we see that the attention mechanism significantly improves the generalization performance of both 2-\acs{gnn} and 2-\acs{wl}-\acs{gnn}.
This indicates that \ac{sampool} is successful at filtering out the randomly generated noisy parts of a given graph and put most attention to the relevant unicolored triangle.

\section{Evaluation on Real-World Data}%
\label{sec:eval:real}
