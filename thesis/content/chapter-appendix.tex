%!TEX root = ../main.tex
% chktex-file 46

\chapter{Appendix}%
\label{sec:appendix}

\section{Evaluated Hyperparameter Grids}%
\label{sec:appendix:config-grid}

To tune the hyperparameters of the evaluated models, we used a regular grid search.
Depending on the type of model, different sets of hyperparameter configurations $\Theta$ were used.

\paragraph{Graph Kernels}
As described in \cref{sec:eval:setup}, we used the \ac{svm} classifier from \citetitle{SKL} to evaluate the graph kernel approaches.
We tuned only the regularization parameter \texttt{C} of this classifier;
the evaluated values are $\mathtt{C} \in \{ 1, \num{1e-1},\allowbreak \num{1e-2},\allowbreak \num{1e-3},\allowbreak \num{1e-4} \}$.
All other parameters were left at the default setting (using \texttt{scikit-learn 0.22.1}).

\paragraph{Baseline and \ac{gin}}
For the evaluation of the structure unaware baseline learner and \ac{gin}, we used the same hyperparameter configurations as \citet{Errica2020}.
We therefore refer to their work for a complete list of the tuned hyperparameters for those models.

\paragraph{2-\acs{gnn} and 2-\acs{wl}-\acs{gnn}}
We evaluated our implementations of 2-\acsp{gnn} as well as 2-\acs{wl}-\acsp{gnn} on the grid spanned by the following hyperparameter values:
\begin{itemize}[itemsep=2pt,parsep=2pt]
	\item \textbf{Number of convolutional layers $T \in \{ 3, 5 \}$:}
		This parameter describes only the depth of the stack of convolutional layers.
		The \ac{mlp} after the pooling layer is always configured with a single hidden layer.
		The \acs{lta}-like configurations of 2-\acs{wl}-\acsp{gnn} are evaluated with the depths $T \in \{ 4, 5 \}$ to compensate for the missing \ac{mlp} after the pooling layer.
	\item \textbf{Layer width $d \in \{ 32, 64 \}$:}
		This parameter describes the output dimensionalities $d = d^{(1)} = \cdots = d^{(T)}$ of the convolutional layers and (if applicable) also the hidden layer width of the final \ac{mlp} after the pooling layer.
		The \acs{lta}-like configurations of 2-\acs{wl}-\acsp{gnn} are evaluated with the same widths but use $d^{(T)} = 1$ in the final layer.
	\item \textbf{Jumping knowledge $\mathrm{JK} \in \{ \mathtt{true}, \mathtt{false} \}$:}
		\citet{Xu2018a} have demonstrated that it can be advantageous for graph classification to pass the outputs of convolutional layers to their successors.
		We incorporate this idea by passing the input feature vectors not only to the first convolution layer but to all convolution layers through concatenation iff.\ $\mathrm{JK} = \mathtt{true}$.
	\item \textbf{Learning rate $\eta \in \{ \num{1e-2}, \num{1e-3}, \num{1e-4} \}$} of the Adam optimizer.
	\item \textbf{Activation functions $\sigma$ and $\sigma_{\Gamma}$} are set to the standard logistic function.
	\item \textbf{Number of epochs $E$ and early stopping patience $p$} are set to $E = 1000$ and $p = 100$, except for the evaluation of the synthetic TRIANGLE dataset for which we used $E = 5000$ and $p = 1000$ to ensure model convergence.
\end{itemize}

\section{Dataset Statistics and Descriptions}%
\label{sec:appendix:ds-stats}

\begin{table}[ht]
	\caption{Sizes of the evaluated binary classification datasets and their graphs.}\label{tbl:appendix:ds-stats}
	\centering\small
	\csvreader[
		tabular={lrrrrrrrrr},
		separator=comma,
		before reading=\setlength{\tabcolsep}{5pt},
		table head={%
			\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & &\multicolumn{3}{c}{vertex count $\left|\mathcal{V}_G\right|$} & \multicolumn{3}{c}{edge count $\left|\mathcal{E}_G\right|$} & \multicolumn{1}{c}{radius} \\%
			& \multirow{-2}{*}[-0.2em]{\shortstack[c]{no.\ of\\ graphs}} & \multirow{-2}{*}[-0.2em]{\shortstack[c]{vertex data\\{\scriptsize (feat.\ + lab.)}}} & $\min$ & $\mean$ & $\max$ & $\min$ & $\mean$ & $\max$ & $\mean \pm\, \sigma$ \\\toprule% chktex 21
		},
		table foot=\bottomrule,
		late after line=\\
	]{data/ds_stats.csv}%
	{name=\name,graph_count=\gcount,%
	node_count_min=\ncountmin,node_count_mean=\ncountmean,node_count_max=\ncountmax,%
	edge_count_min=\ecountmin,edge_count_mean=\ecountmean,edge_count_max=\ecountmax,%
	node_degree_min=\ndegmin,node_degree_mean=\ndegmean,node_degree_max=\ndegmax,%
	dim_node_features=\nfdim,dim_edge_features=\efdim, radius_mean=\radiusmean, radius_std=\radiusstd%
	}%
	{\textbf{\name}&%
	$\gcount$&%
	$\nfdim$&%
	$\ncountmin$&$\ncountmean$&$\ncountmax$&%
	$\ecountmin$&$\ecountmean$&$\ecountmax$&$\radiusmean \pm \radiusstd$%
	}
\end{table}

\paragraph{TRIANGLE}
The triangle detection dataset was generated by sampling three graphs with exactly one unicolored triangle uniformly at random for each possible combination of the following parameters:
The number of vertices (between 6 and 32), the vertex color proportions (either 50/50\%, 75/25\% or 25/75\% vertices with the colors \colorlabel{t_blue}{A}/\colorlabel{t_red}{B}), the graph density (${\left| \mathcal{V}_{G} \right|}^{-2} \left| \mathcal{E}_{G} \right| \in \{ \nicefrac{1}{4}, \nicefrac{1}{2} \}$) the graph class (add a triangle with either the color \colorlabel{t_blue}{A} or \colorlabel{t_red}{B}).

\paragraph{NCI1}
This dataset was made available by \citet{Shervashidze2011}.
It contains a balanced subset of molecule graphs that were originally published by the US \ac{nci}~\cite{Wale2007}.
In each molecule graph, vertices correspond to atoms and edges to bonds between them.
The binary classes in this dataset describe whether a molecule is able to suppress or inhibit the growth of certain lung cancer and ovarian cancer cell lines in humans.

\paragraph{PROTEINS and D\&D}
The graphs in both the PROTEINS dataset~\cite{Borgwardt2005a} as well as the D\&D dataset~\cite{Dobson2003} represent proteins.
Each vertex corresponds to a so-called \ac{sse}, i.e.\ a certain molecular substructure.
An edge encodes either that two \acp{sse} are neighbors in the protein's amino-acid sequence or that those \acp{sse} are close to each other in 3D space.
Each protein graph is classified by whether it is an enzyme or not.
The main difference between the two datasets is their selection of vertex features/labels.

\paragraph{REDDIT}
This balanced dataset contains graphs that represent online discussion threads on the website Reddit~\cite{Yanardag2015}.
Each vertex corresponds to a user; an edge is drawn between two users iff.\ at least one of them replied to a comment of another.
Such social interaction graphs were sampled from two types of subreddits:
Question/answer-based and discussion-based.
The classification goal is to predict from which type of subreddit a given graph was sampled.

\paragraph{IMDB}
This dataset contains so-called \textit{ego-networks} of movie actors~\cite{Yanardag2015}.
Vertices in such networks represent actors and edges encode whether two actors starred in the same movie.
The graphs in the dataset are derived from the actors starring in either action or romance movies.
The classification goal for each graph is to predict the movie genre it was derived from.

\section{Fold-wise Accuracy Deltas}%
\label{sec:appendix:fold-diffs}

Due to the relatively small sizes of the evaluated benchmark datasets, the variance of the test accuracies across different folds is quite large.
When directly comparing the mean accuracies of two learners, it is therefore often impossible to tell whether one consistently outperforms the other.
We therefore now list the mean and standard deviations of the fold-wise test accuracy differences of all pairs of learners for all datasets.
This effectively removes the variance introduced by ``easy'' and ``hard'' folds on which all learners might tend to perform consistently better/worse.

In the following \crefrange{tbl:appendix:diff-triangle}{tbl:appendix:diff-imdb} (\cpagerefrange{tbl:appendix:diff-triangle}{tbl:appendix:diff-imdb}), the differences are computed as $\textit{row accuracy} - \textit{column accuracy}$.
For each row $i$ and column $j$ we highlight the corresponding cell $(i,j)$ in \textcolor{t_red}{red} or \textcolor{t_darkgreen}{green} iff.\ the learner~$i$ performs consistently \textcolor{t_red}{worse} (or \textcolor{t_darkgreen}{better} respectively) than $j$ with a significance level of $2\sigma$.
To compute the deltas for 2-\acs{wl}-\acsp{gnn}, we use the same neighborhood radii as in \cref{tbl:eval:synthetic,tbl:eval:real}, i.e.\ $r = 2$ for the synthetic triangle detection dataset and $r = 8$, $5$, $2$, $1$ and $4$ for NCI1, PROTEINS, D\&D, REDDIT and IMDB respectively.

{\captionsetup[table]{name=Matrix}%
\begin{table}[ht]
	\caption{Fold-wise accuracy delta means and standard deviations on the triangle dataset.}\label[mat]{tbl:appendix:diff-triangle}
	\centering
	\input{data/diffs/triangle.tex}
\end{table}
\begin{table}[ht]
	\caption{Fold-wise accuracy delta means and standard deviations on NCI1.}\label[mat]{tbl:appendix:diff-nci}
	\centering
	\input{data/diffs/nci.tex}
\end{table}
\begin{table}[ht]
	\caption{Fold-wise accuracy delta means and standard deviations on PROTEINS.}\label[mat]{tbl:appendix:diff-proteins}
	\centering
	\input{data/diffs/proteins.tex}
\end{table}
\begin{table}[ht]
	\caption{Fold-wise accuracy delta means and standard deviations on D\&D.}\label[mat]{tbl:appendix:diff-dd}
	\centering
	\input{data/diffs/dd.tex}
\end{table}
\begin{table}[ht]
	\caption{Fold-wise accuracy delta means and standard deviations on REDDIT.}\label[mat]{tbl:appendix:diff-reddit}
	\centering
	\input{data/diffs/reddit.tex}
\end{table}
\begin{table}[ht]
	\caption{Fold-wise accuracy delta means and standard deviations on IMDB.}\label[mat]{tbl:appendix:diff-imdb}
	\centering
	\input{data/diffs/imdb.tex}
\end{table}%
}
