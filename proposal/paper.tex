% chktex-file 46
\documentclass[12pt]{scrartcl}

\PassOptionsToPackage{utf8}{inputenc}
\usepackage{inputenc}
\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage{microtype}

\usepackage{graphicx}
\graphicspath{{images/}}

\usepackage{paralist}
\usepackage{csquotes}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{geometry}
% \geometry{a4paper,body={5.8in,9in}}
\geometry{a4paper}
\renewcommand{\baselinestretch}{1.1}
\usepackage{parskip}
\setlength{\parindent}{0pt}

\usepackage{amsmath, amsfonts, amssymb}
\usepackage{bm}
\usepackage{placeins}
\usepackage{subcaption}

\usepackage{setspace}

\usepackage{hyperref}
\usepackage[nameinlink]{cleveref}
\newcommand{\crefrangeconjunction}{--}

\usepackage[						% use biblatex for bibliography
	backend=bibtex,					% 	- use biber backend (bibtex replacement) or bibtex
	style=numeric,					% 	- use alphabetic (or numeric) bib style
	natbib=true,					% 	- allow natbib commands
	hyperref=true,					% 	- activate hyperref support
	backref=true,					% 	- activate backrefs
	isbn=false,						% 	- don't show isbn tags
	url=false,						% 	- don't show url tags
	doi=false,						% 	- don't show doi tags
	urldate=long,					% 	- display type for dates
	maxnames=3,%
	minnames=1,%
	maxbibnames=5,%
	minbibnames=3,%
	maxcitenames=2,%
	mincitenames=1,%
	sorting=none
]{biblatex}
\bibliography{literature}

\usepackage[inline]{enumitem}
\usepackage{todonotes}
\usepackage{pgfgantt}

% Colors:
\definecolor{blau}{HTML}{355FB3}
\definecolor{rot}{HTML}{B33535}
\definecolor{gruen}{HTML}{3BB335}
\definecolor{hellblau}{HTML}{8ea7d7}
\definecolor{hellgrau}{HTML}{cccccc}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\thesisTitle}{Learning to Aggregate on Structured Data}
\newcommand{\thesisSubject}{Master Thesis Proposal \& Work Plan}
\newcommand{\thesisName}{Clemens Damke}
\newcommand{\thesisMail}{cdamke@mail.uni-paderborn.de}
\newcommand{\thesisMatNr}{7011488}
\hypersetup{% setup the hyperref-package options
    pdftitle={\thesisTitle},    %   - title (PDF meta)
    pdfsubject={\thesisSubject},%   - subject (PDF meta)
    pdfauthor={\thesisName},    %   - author (PDF meta)
    plainpages=false,           %   -
    colorlinks=false,           %   - colorize links?
    pdfborder={0 0 0},          %   -
    breaklinks=true,            %   - allow line break inside links
    bookmarksnumbered=true,     %
    bookmarksopen=true          %
}

\begin{document}

\title{\thesisTitle}
\subtitle{\thesisSubject}
\author{{\thesisName}\\\small{Matriculation Number: \thesisMatNr}\\\small{\href{mailto:\thesisMail}{\thesisMail}}}
\date{\today}
\maketitle

\section{Motivation}%
\label{sec:motivation}

Most of the commonly used supervised machine learning techniques assume that instances are represented by $d$-dimensional feature vectors $x_i \in \mathcal{X} = \mathcal{X}_1 \times \cdots \times \mathcal{X}_d$ for which some target value $y_i \in \mathcal{Y}$ should be predicted.
In the regression setting the target domain $\mathcal{Y}$ is continuous, typically $\mathcal{Y} = \mathbb{R}$, whereas $\mathcal{Y}$ is some discrete set of classes in the classification setting.

Since not all data is well-suited for a fixed-dimensional vector representation, approaches that directly consider the structure of the input data might be more appropriate in such cases.
One such case is the class of so-called \textit{learning to aggregate} (LTA) problems as described by \citet{Melnikov2016}.
There the instances are represented by compositions $\bm{c}_i$ of constituents $c_{i,j} \in \bm{c}_i$, i.e.\@ variable-size multisets with $n_i = |\bm{c}_i|$.
The assumption in LTA problems is that for all constituents $c_{i,j}$ a local score $y_{i,j} \in \mathcal{Y}$ is either given or computable.
The set of those local scores should be indicative of the overall score $y_i \in \mathcal{Y}$ of the composition $\bm{c}_i$.
LTA problems typically require two subproblems to be solved:
\begin{enumerate}[label=\textbf{\arabic*.}]
	\item \textbf{Aggregation:}
		A variadic aggregation function $A: \mathcal{Y}^{*} \to \mathcal{Y}$ that estimates composite scores has to be learned, i.e.\@ $y_i \approx \hat{y}_i = A(y_{i,1}, \dots, y_{i,n_i})$.
		Typically the aggregation function $A$ should be associative and commutative to fit with the multiset-structure of compositions.
	\item \textbf{Disaggregation:}
		In case the constituent scores $y_{i,j}$ are not given, they have to be derived from a constituent representation, e.g.\ a vector $v_{i,j} \in \mathcal{V}$.
		To learn this derivation function $f: \mathcal{V} \to \mathcal{Y}$, only the constituent vectors ${\{v_{i,j}\}}_{j = 1}^{n_i}$ and the composite score $y_i$ is given.
		Thus the constituent scores $y_{i,j}$ need to be \textit{disaggregated} from $y_i$ in order to learn $f$.
\end{enumerate}
Overall LTA can be understood as the joint problem of learning the aggregation function $A$ and the local score derivation function $f$.

Current LTA approaches only work with multiset inputs.
In practice there is however often some relational structure among the constituents of a composition.
This effectively turns LTA into a graph regression problem.
The goal of this thesis is to look into the question of how aggregation function learning methods might be generalized to the graph setting.

\section{Related Work}%
\label{sec:related-work}

This thesis will be based on two currently mostly separate fields of research:
\begin{enumerate*}[label=\textbf{\arabic*.}]
	\item Learning to Aggregate
	\item Graph classification \& regression
\end{enumerate*}.
A short overview of the current state-of-the-art approaches in both fields will be given now.

\subsection{Learning to Aggregate}%
\label{sec:related-work:lta}

Two main approaches to represent the aggregation function in LTA problems have been explored.
The first approach uses \textit{uninorms}~\cite{Melnikov2016} to do so.
There the basic idea is to express composite scores as fuzzy truth assignments $y_i \in [0, 1]$.
Such a composite assignment $y_i$ is modeled as the result of a parameterized logical expression of constituent assignments $y_{i,j} \in [0, 1]$.
As the logical expression that thus effectively aggregates the constituents, a uninorm $U_{\lambda}$ is used.
Depending on the parameter $\lambda$, $U_{\lambda}$ combines t-norms and t-conorms which are continuous generalizations of logical conjunction and disjunction respectively.

Recently \citet{Melnikov2019} have also looked at an alternative class of aggregation functions.
Instead of using fuzzy logic to describe score aggregation, \textit{ordered weighted average} (OWA) operators were used.
OWA aggregators work by sorting the input scores and then weighting them based on their sort position, i.e.\ %
\begin{align*}
	A_{\lambda}(y_1, \dots, y_n) := \sum_{i = 1}^n \lambda_i y_{\pi(i)},
\end{align*}
where $\lambda$ is a weight vector with ${\|\lambda\|}_1 = 1$ and $\pi$ is a sorting permutation of the input scores. % chktex 21
To deal with varying composite sizes $n$ the weights $\lambda_i$ are interpolated using a \textit{basic unit interval monotone} (BUM) function $q: [0, 1] \to [0, 1]$.
It takes constituent positions that are normalized to the unit interval, i.e.\ $\frac{i}{n}$.
The BUM function $q$ then is then used to interpolate a weight for any normalized sort position via $\lambda_i = q(\frac{i}{n}) - q(\frac{i - 1}{n})$.
Therefore the learning objective boils down to optimizing the shape of $q$.
The details of this are left out here.

\subsection{Graph Classification and Regression}%
\label{sec:related-work:gcr}

As previously mentioned, the addition of relations between constituents turns LTA into a graph regression problem.
Most of the recent research in the field of learning from graph structured data has however focused on the closely related graph classification problem.
Since many ideas from the classification setting are also useful in the regression setting, a brief overview of those ideas is given first.

At a high level graph classification methods can be taxonomized into two main families:
\begin{enumerate}[label=\textbf{\arabic*.}]
	\item \textbf{Vector representation approaches:}
		One way to tackle the graph classification problem is to map an input graph $G$ to a vectorial representation.
		This can be done
		\begin{enumerate*}
			\item by either handpicking global graph features like vertex/edge count, degree distribution or graph diameter,
			\item via a graph embedding algorithm like node2vec~\cite{Grover2016}, sub2vec~\cite{Adhikari2018} or graph2vec~\cite{Narayanan2017},
			\item implicitly by using a graph kernel that computes the structural similarity between graphs, e.g.\ the Weisfeiler-Lehman kernel~\cite{Shervashidze2011} or the multiscale Laplacian graph kernel~\cite{Kondor2016}
		\end{enumerate*}.
		Graphs can then be classified via any classification algorithm that works with vectors and/or kernels.
	\item \textbf{Graph neural networks:}
		An alternative approach is to adapt neural networks to graph inputs.
		The notion of a \textit{graph neural network} (GNN) was first introduced by \citet{Gori2005}.\@
		There a message-passing architecture is used to iteratively propagate vertex information to neighbors until a fixed point is reached.
		This process is computationally expensive.

		Recently the class of \textit{convolutional graph neural networks} (ConvGNNs) gained traction.
		It generalizes the convolution operator that is used by \textit{convolutional neural networks} (CNNs) to graphs.
		There are two main variants of ConvGNNs:
		\begin{enumerate}[label=\textbf{\alph*)}] % chktex 9 chktex 10
			\item \textbf{Spatial variant:}
				Standard CNNs use a convolution operator that is typically applied to a grid graph of pixels with added diagonal edges.
				Spatial ConvGNNs~\cite{Micheli2009} directly generalize this idea by defining a more flexible convolution that works with arbitrary graph structures.
				Spatial graph convolution aggregates the direct neighborhood of a vertex $v_i$, where $v_i$ is typically described by a feature vector $x_i \in \mathbb{R}^d$.
				This process returns a new aggregate feature vector $x'_i$.
				By stacking multiple convolution layers, features from indirect neighbors become part of the aggregate.
				Analogous to how CNNs learn kernel matrices, spatial ConvGNNs learn vertex neighborhood feature aggregation functions.
				This approach shares similarities with the previously mentioned message-passing architecture~\cite{Gori2005} with the major different being that the number of message-passing, i.e.\ convolution, steps is fixed.
			\item \textbf{Spectral variant:}
				Motivated by the theoretical foundation provided by spectral graph theory~\cite{Shuman2013}, spectral ConvGNNs~\cite{Bruna2013} learn a filter function $\hat{g}$ that is applied in the Fourier domain of a graph.
				Based on the convolutional theorem, a convolution operator can not only be expressed in terms of the neighborhood of vertices but also as a filter on the eigenvectors of a graph's Laplacian.
				Formally this can be expressed as
				\begin{align}
					g *_G \bm{x} = U \hat{g}(\Lambda) U^{\top} \bm{x}\label{eq:related-work:conv-filter}
				\end{align}
				where $\bm{x} \in \mathbb{R}^{n \times k}$ is the matrix of all vertex features and $U \Lambda U^{\top}$ is the eigendecomposition of the graph Laplacian $L_G$\footnote{
					To see the connection to the convolutional theorem, note that $\mathcal{F} = U$ and $\mathcal{F}^{-1} = U^{\top}$ in \cref{eq:related-work:conv-filter} with $\mathcal{F}$ denoting the graph Fourier transform.
				}.
				In the previously described spatial ConvGNN variant, $g$ is learned directly in form of the feature aggregation function with a neighborhood locality constraint.
				In the spectral variant however the Fourier-transformed filter $\hat{g}$ is learned.
				Intuitively this means that vertex features $x_i$ are not aggregated with the features of their their direct neighborhood but with the features of vertices with which they share certain structural similarities.
				This allows spectral ConvGNNs to incorporate the global graph structure at the cost of having to perform the computationally expensive decomposition of every input graph's Laplacian.

				To tackle the performance impact of the spectral convolution approach, various simplifications of the filter $\hat{g}$ have been proposed.
				The so-called \textit{graph convolutional network}~(GCN)~\cite{Kipf2017} does this by reducing the expressivity of the filter.
				Then $\hat{g}$ can be applied to the Laplacian directly via $g *_G \bm{x} = \hat{g}(L_G) \bm{x}$ which saves the decomposition costs.
				This simplification of $\hat{g}$ implicitly causes its Fourier inverse $g$ to be locality constrained just like in the spatial ConvGNN variant.
				Hence the spectral GCN approach also allows for a spatial/message-passing interpretation.
				Recently various variants and extensions of GCNs~\cite{Hamilton2017}\cite{Chen2018}\cite{Chen2017}\cite{Chiang2019}\cite{Du2017} have been proposed that apply additional approximations and sampling methods to improve the runtime as well as the accuracy in certain scenarios.

				One notable extension of GCNs are the so-called \textit{adadptive graph convolutional networks} (AGCNs)~\cite{Li2018}.
				AGCNs do not use a fixed Laplacian $L$ for each input but instead learn a so-called residual Laplacian $L_\mathit{res}$.
				The residual Laplacian is constructed by learning a vertex similarity kernel $\mathbb{G}_{x_i,x_j}$ which is used to determine edge weights between vertices.
				By overlaying the learned residual Laplacian $L_\mathit{res}$ on top of the given intrinsic Laplacian $L$, similar nodes with a large spatial distance in $L$ become connected.
				This can improve the accuracy especially if a locality constrained spectral filter $\hat{g}$ is used.
		\end{enumerate}

		The just described ConvGNN approaches generally assume that an input consists of an undirected graph with positive real edge weights.
		Those approaches are not directly applicable in scenarios with multiple edge types, i.e.\ multi-relational scenarios.
		One class of spatial multi-relational GNN methods was proposed by \citet{Battaglia2018}.
		They describe a general framework for the construction of GNNs; for simplicity only the class message-passing variants is mentioned here.
		There a shared function $\phi^e$ is used to describe how a vertex feature vector $v_i$ is transformed as it passes through an edge with features $e_{ij} \in \mathbb{R}^p$.
		In a uni-relational spatial GNN with $e_{ij} \in \mathbb{R}^{1}$, typically $\phi^e(e_{ij}, v_i) = e_k \cdot v_i$ is used.
		In the multi-relational setting, $\phi^e$ can instead be learned via a standard \textit{multilayer perceptron} (MLP).
		An alternative spectral multi-relational ConvGNN approach was proposed by \citet{Gong2018}.
		Their so-called \textit{edge enhanced graph neural network} (EGNN) essentially models each relation as a separate graph with its own Laplacian.
		A convolutional filter $g$ is shared across all relations and used to convolve the input signal separately via each relation's Laplacian.
		To share vertex features across relations, the separately convolved signals of all relations are then concatenated.
		Those concatenated vertex features are used as the input signal of the next layer.

		This concludes the short overview of approaches to learn vertex feature vectors via ConvGNNs.
		After the application of either spatial or spectral graph convolutions, each vertex will have an updated aggregate feature vector.
		To obtain an aggregate score for the entire graph, a graph pooling layer is applied.
		A primitive pooling approach is to simply merge the vertex features via a fixed aggregation function like $\min$, $\max$ or $\text{avg}$.
		Better results can be obtained by learning the pooling aggregation function.
		Two notable graph pooling approaches are \textit{SortPooling}~\cite{Zhang2018} and \textit{Self-Attention Pooling}~\cite{Lee2019}.
		\begin{enumerate}
			\item SortPooling assumes that the vertex feature vectors are obtained using a stack of spectral convolution layers that learn filters on the random walk Laplacian $L = I - D^{-1} A$.
				This assumption makes it possible to interpret the outputs of the graph convolution layers as a continuous generalization of \textit{Weisfeiler-Lehman} (WL) colors.
				WL colors are lexicographically sortable signatures that encode the structural role of vertices; they can be used for efficient graph isomorphism falsification~\cite{Weisfeiler1968}.
				Using this interpretation of the vertex feature vectors, SortPooling determines a top-$k$ ranking of the vertices' WL colors and then aggregates the thus selected fixed-size vertex feature subset via a MLP.\@
			\item Self-Attention Pooling adds an additional graph convolution layer after an arbitrary ConvGNN.\@
				The added layer takes the aggregated vertex feature vectors as input and outputs a score for each vertex.
				This score is used to determine a top-$k$ vertex ranking.
				Unlike SortPooling the feature vectors of the resulting subset are first added up.
				The resulting aggregate graph feature vector is then fed into a MLP to obtain a class.
		\end{enumerate}
\end{enumerate}

While all of the previously mentioned problems were evaluated on graph classification problems, they can naturally be extended to regression tasks.
In the case of ConvGNNs for example, the described pooling layers already produce continuous outputs.

The pooling aggregation functions used in ConvGNNs are however typically not part of the learning objective.
This shows the fundamental methodological difference between GNNs and LTA.\@
A GNN uses the relational structure of its input to try to determine meaningful feature representations or scores for each vertex.
Those vertex scores are then combined using a simple aggregation function.
LTA on the other hand currently does not use any relational structure and therefore computes vertex/constituent scores independently.
Those comparatively less meaningful local scores are then combined using a more expressive learned aggregation function.
Generally speaking current GNN approaches focus on the disaggregation problem while LTA approaches focus on the aggregation problem.
The described Self-Attention Pooling~\cite{Lee2019} can be seen as an exception to this generalization since it learns an aggregation function in form of the self-attention vertex weights.

\subsection{Molecular Analysis Problems and Datasets}%
\label{sec:related-work:tox}

Graph regression problems have for the most part only been discussed in the context of specific domain problems.
One of those domains is the prediction of chemical toxicity, typically the $\text{LD}_{50}$ or $\text{LC}_{50}$ doses which measure the smallest amount of a given chemical that is lethal for at least $50\%$ of tested subjects.
Another regression task from the domain of molecular analysis is the prediction of compound solubility.
The following approaches could serve as inspiration, baselines and/or sources of training data:
\begin{itemize}
	\item \textit{RASAR}~\cite{Luechtefeld2018} uses a logistic regression model.
		It is trained with manually chosen features that are given for training molecule graphs.
		To predict the toxicity of a new chemical, $k$-means is used to average the features of chemicals with similar molecule graphs in the training set.
		The structural similarity between molecule graphs is measured by the Tanimoto coefficient of their chemical fingerprints.
		Those fingerprints encode the presence or absence of certain meaningful chemical substructures.
		The implementation and used dataset is unfortunately not publicly available because it is part of a commercial product\footnote{ToxTrack Inc., \url{https://toxtrack.com/}}.
	\item \textit{ProTox}~\cite{Drwal2014}\cite{Banerjee2018} is another toxicity prediction approach.
		It works similarly to RASAR but leaves out the logistic regression step and instead directly aggregates the toxicity of similar chemicals via $k$-means.
		ProTox uses the freely available \textit{SuperToxic} dataset~\cite{Schmidt2009} which contains 13400 molecules with known $\text{LD}_{50}$ values.
		While the implementation is again not publicly available, the trained model can be freely queried online\footnote{ProTox-II, \url{http://tox.charite.de/protox_II/}}.
	\item The \textit{Toxicity Estimation Software Tools}~(T.E.S.T.)\footnote{\url{https://www.epa.gov/chemical-research/toxicity-estimation-software-tool-test}} is a free collection of models for toxicity prediction.
		Like RASAR and ProTox it includes similarity-based prediction models but also linear regression and decision tree models that are trained on vector representations of chemicals.
		The publicly available \textit{ChemIDplus}\footnote{\url{https://chem.nlm.nih.gov/chemidplus/}} database was used for training.
		It contains 28023 and 13541 molecules with known oral $\text{LD}_{50}$ values for rats and mice respectively.
		Additionally about 1000 molecules with known $\text{LC}_{50}$ values are available for both species.
	\item Recently first toxicity prediction approaches using ConvGNNs have been published.
		The previously mentioned AGCN~\cite{Li2018} was in fact evaluated on a discrete toxicity classification problem;
		this could serve as a starting point for a continuous graph regression method.
		\citet{Pope2018} describe how functional groups of toxic chemicals can be learned using GCNs;
		there a functional group refers to a relevant subgraph of a molecule that can be interpreted as a toxicity indicator.
	\item The previously described EGNN method~\cite{Gong2018} for multi-relational graphs was evaluated on a compound solubility prediction task.
		For this the Lipophil\-icity~\cite{Wu2017} and FreeSolv~\cite{Mobley2014} datasets were used.
		They contain 4200 and 642 labeled molecular graphs respectively.
\end{itemize}

\section{Research Question}%
\label{sec:question}

\todo[inline,color=green]{Since the final research question is not yet fixed, I only sketched out the main candidates.}

\todo[inline]{Candidate 1: Comparison of LTA and GNNs}
As described, current graph classification and regression approaches mostly use simple fixed aggregation functions like $\min$, $\max$ or $\text{avg}$.
Self-Attention Pooling~\cite{Lee2019} is a notable exception that learns the weights for a weighted average that is computed over convolved vertex features.
An open question for research is whether more complex aggregation functions are able to outperform simple aggregation functions in the graph setting.
\todo[inline,color=green]{Not sure how to approach this yet.}

\todo[inline]{Candidate 2: Structural Restrictions}
The previously described graph classification and regression methods assume that arbitrary graphs are provided as input.
The question this thesis aims to answer is whether a restriction of the input graph structure makes it possible to use a more specialized method that outperforms the more general approaches.
Possible restrictions to look at could for example be vertex degree or tree structure constraints.\todo[inline,color=green]{
	I suspect that such restrictions will mostly improve upon runtime (either automatically because most GNN methods depend on $\mathcal{O}(\left|E\right|)$ or because expensive matrix multiplications can be replaced).
	Those runtime savings might then of course be translated into regression performance improvements, e.g.\ maybe by using a deeper model.
	As we discussed a restriction to tree structures may make hierarchical aggregation functions feasible but as I understand those can in principle be expressed via a message-passing GNN as well~\cite{Battaglia2018}.
}

\todo[inline]{Candidate 3: Multi-relational ConvGNNs}
As mentioned in \cref{sec:related-work:gcr}, most ConvGNN variants are not directly applicable to multi-relational inputs.
Two more flexible approaches were described:
\begin{enumerate}
	\item The general graph network framework described by \citet{Battaglia2018} that can be interpreted as a spatial message-passing approach.
		They share information across relations via an arbitrary learned function $\phi^e(e_{ij}, e_i)$.
	\item EGNNs~\cite{Gong2018}, a spectral ConvGNN variant.
		They share information across relations by concatenating vertex features that are separately convolved in each relation's graph.
\end{enumerate}
The first approach does not discuss which specific choice of edge propagation function $\phi^e$ is suitable for a multi-relational regression problem like those described in \cref{sec:related-work:tox}.
In the second approach the concatenation operator implicitly acts as the edge propagation function.
As we will see in \cref{sec:approach}, this assumes that every relation shares information with the other relations in the same way.
Currently there are no approaches that explicitly model the correlation structure between relations, i.e.\ if and how information propagates from one relation to another.
The research question for this thesis is to find out how a model that incorporates cross-relational correlations might look like and to compare it theoretically and empirically with another multi-relational method like the EGNN.\@

\section{Approach}%
\label{sec:approach}

\todo[inline,color=green]{As discussed on Monday, I added a rough overview of the formal background for candidate 3.}
A possible approach to define a multi-relational ConvGNN is to directly extend an existing method, e.g.\ GCNs~\cite{Kipf2017}, from the one-dimensional to the multi-dimensional setting.
Typically a graph is modeled via a symmetric adjacency matrix $E \in \mathbb{R}^{N \times N}$ with non-negative edge weights $e_{ij} = e_{ji} \in \mathbb{R}$.
An edge weight $e_{ij}$ can be interpreted as a member of the multiplicative group on $\mathbb{R}$ and encodes a spatial signal strength scaling operation between the vertices $i$ and $j$.
If we now go from a single relation to $M$ relations, the operator encoded by an edge can be generalized from $e_{ij} \in \mathbb{R}^{1 \times 1}$ to $\bm{e}_{ij} \in \mathbb{R}^{M \times M}$.
The non-negativity constraint $e_{ij} \geq 0$ on the edge weights then generalizes to a positive semidefiniteness constraint $\bm{e}_{ij} \succeq 0$ on the edge matrices; therefore $\bm{e}_{ij}$ is symmetric.
$\bm{e}_{ijkl} = \bm{e}_{ijlk} = \bm{e}_{jikl} = \bm{e}_{jilk} \in \mathbb{R}$ describes the connection strength between the signals of the relations $k$ and $l$ at vertex $i$ and the signals of the same relations at vertex $j$.

Using multi-dimensional edge operators as described above turns $E \in \mathbb{R}^{(N \cdot M) \times (N \cdot M)} \equiv {(\mathbb{R}^{M \times M})}^{N \times N} \equiv \mathbb{R}^{N \times N} \otimes \mathbb{R}^{M \times M}$ into a $(2,2)$-tensor.
The notion of a one-dimensional graph signal $x \in \mathbb{R}^{N}$ then generalizes to $x \in \mathbb{R}^{N \cdot M} \equiv {(\mathbb{R}^M)}^N \equiv \mathbb{R}^N \otimes \mathbb{R}^M$, a $(2, 0)$-tensor that describes the signal strength of each relation at each vertex.

Another way to look at the resulting adjacency structure is as a linear operator $E: \mathbb{R}^{N \cdot M} \to \mathbb{R}^{N \cdot M}$ that acts on the $N$-dimensional module over the division ring $R$ of positive semidefinite real $M \times M$ matrices.\todo{TODO:\@ Formalize the inverse and missing covariant factor.}

To motivate a ConvGNN on this more abstract definition of $E$ we first define the multi-relational combinatorial Laplacian $\mathcal{L} := D - E$ via $D_{ii} := \sum \bm{e}_{ii}$ and $D_{ij} := \bm{0}$ for $i \neq j$, i.e.\ $D$ is a block matrix with summed $M \times M$ blocks on its diagonal and zeros everywhere else.
The normalized multi-relational Laplacian can then be defined as $L := I - D^{-\frac{1}{2}} E D^{-\frac{1}{2}}$ using the fact that all $D_{ii}$ are elements of the division ring $R$.
A spectral filter $\hat{g}$ can therefore be learned for $L$, analogous to GCNs~\cite{Kipf2017}.

The main remaining question now is how the entries of the edge operators $\bm{e}_{ij} \in \mathbb{R}^{M \times M}$ are determined.
In uni-relational methods the scalar edge weight $e_{ij} \in \mathbb{R}$ is typically assumed to be given for each input.
In the multi-relational setting we can assume that a scalar connection strength is given for each relation, i.e.\ the self-correlations $\bm{e}_{ijkk} \in \mathbb{R}$ on the diagonal are known.
The off-diagonal entries of an edge operator however represent the cross-relational correlations and cannot be assumed to be known.
If we set the off-diagonal entries to zero, i.e.\ $\forall i, j, k, l: k \neq l \rightarrow e_{ijkl} = 0$, all relations would be assumed to be uncorrelated and $M$ spectral filters would be learned independently on the graphs that are described by the relations.\todo{TODO:\@ Add a formal proof?}

Since we cannot generally assume that all relations are uncorrelated, a \textit{relational correlation kernel} $r$ has to be learned together with the spectral filter $\hat{g}$.
$r$ should describe how strong one relation is correlated to another and is used to compute off-diagonal edge operator entries.

The previously mentioned EGNNs~\cite{Gong2018} can in fact be interpreted using the notion of edge operators.
They implicitly assume that the rows in all $e_{ij}$ are equal, i.e.\ the off-diagonal entries are set to\todo{TODO:\@ Explain this.}
\begin{align*}
	\begin{pmatrix}
		\bm{e}_{ij11} & & \\
		& \ddots & \\
		& & \bm{e}_{ijMM}
	\end{pmatrix} \rightarrow \begin{pmatrix}
		\bm{e}_{ij11} & \cdots & \bm{e}_{ijMM} \\
		\vdots & \vdots & \vdots \\
		\bm{e}_{ij11} & \cdots & \bm{e}_{ijMM}
	\end{pmatrix}\text{.}
\end{align*}
A more flexible correlation structure can potentially yield an improvement over the current multi-relational approaches.

One problem that arises from the described approach is that the adjacency tensor $E$ is changed after each change of $r$ and therefore $\mathcal{L}$ needs to be renormalized after every training step.
This can potentially incur a significant performance cost and should be mitigated by employing results from perturbation theory.\todo{TODO:\@ Elaborate.}

\section{Preliminary Document Structure}%
\label{sec:doc-structure}

\begin{enumerate}
	\item Introduction
	\item Previous Work
		\begin{enumerate}[label*=\arabic*.]
			\item Learning to Aggregate
			\item Convolutional Graph Neural Networks
		\end{enumerate}
	\item Learning to Aggregate Graphs
	\begin{enumerate}[label*=\arabic*.]
		\item A Vertex Aggregation Function Learner
		\item Compatible Disaggregation Methods
	\end{enumerate}
	\item Evaluation
		\begin{enumerate}[label*=\arabic*.]
			\item Learning Characteristics of Generated Graphs
			\item Learning Chemical Toxicity
			\item Scalability
		\end{enumerate}
	\item Conclusion
	\item Literature
	\item Appendix
\end{enumerate}

\section{Time-Schedule}%
\label{sec:schedule}

\begin{figure}[!ht]
	\centering
	\begin{ganttchart}[
		x unit = 0.55cm,
		y unit title = 0.6cm,
		y unit chart = 0.7cm,
		vgrid,
		canvas/.style = {
			draw = none
		},
		title label font = \sffamily,
		title height = 1,
		title/.style = {
			draw = none,
			fill = none
		},
		group label font = \sffamily\bfseries,
		group label node/.append style = {
			left = 0.2cm
		},
		group/.append style = {
			fill = blau,
			draw = none
		},
		group peaks tip position = 0,
		bar label font = \sffamily\bfseries,
		bar label node/.append style = {
			left = 0.2cm
		},
		bar/.append style = {
			fill = blau,
			draw = none
		}
	]{1}{20}
		\gantttitle{week}{20} \\
		\gantttitlelist{1,...,20}{1} \\ % ignore triple dots: chktex 11

		\ganttgroup{Implementation}{1}{11} \\
		\ganttbar[bar label font = \sffamily, bar/.append style = {fill = hellblau}]
			{Data Preparation}{1}{1} \\
		\ganttbar[bar label font = \sffamily, bar/.append style = {fill = hellblau}]
			{Basic ConvGNN}{2}{4} \\
		\ganttbar[bar label font = \sffamily, bar/.append style = {fill = hellblau}]
			{Aggregation Learner}{5}{8} \\
			\ganttbar[bar label font = \sffamily, bar/.append style = {fill = hellgrau}]
				{Buffer}{9}{9} \\
		\ganttbar[bar label font = \sffamily, bar/.append style = {fill = hellblau}]
			{Evaluation}{10}{11} \\

		\ganttgroup{Writing}{2}{19} \\
		\ganttbar[bar label font = \sffamily, bar/.append style = {fill = hellblau}]
			{Previous Work}{2}{5} \\
		\ganttbar[bar label font = \sffamily, bar/.append style = {fill = hellblau}]
			{L.t.A. Graphs}{6}{10} \\
		\ganttbar[bar label font = \sffamily, bar/.append style = {fill = hellblau}]
			{Evaluation}{11}{13} \\
		\ganttbar[bar label font = \sffamily, bar/.append style = {fill = hellblau}]
			{Conclusion}{14}{14} \\
		\ganttbar[bar label font = \sffamily, bar/.append style = {fill = hellgrau}]
			{Buffer}{15}{17} \\
		\ganttbar[bar label font = \sffamily, bar/.append style = {fill = hellblau}]
			{Corrections}{18}{19} \\

		\ganttbar{Presentation}{19}{20}
	\end{ganttchart}
	\caption{Sketch of the time schedule for the work on the thesis}\label{fig:schedule}
\end{figure}

% % %
\newpage
{%
\renewcommand{\bibfont}{\normalfont\small}
\setlength{\biblabelsep}{5pt}
\setlength{\bibitemsep}{0.5\baselineskip plus 0.5\baselineskip} % chktex 1
\setcounter{biburllcpenalty}{9000}
\setcounter{biburlucpenalty}{9999}
\printbibliography%
}

\null\vfill

\begin{center}
	\begin{tabular}{l p{0.1\textwidth} r}
		\cline{1-1} \cline{3-3}
		\begin{minipage}[t]{0.4\textwidth}
			\centering
			\vspace{0cm}Supervisor
		\end{minipage}
		&
		\begin{minipage}[t]{0.2\textwidth}
		\end{minipage}
		&
		\begin{minipage}[t]{0.4\textwidth}
			\centering
			\vspace{0cm}Student
		\end{minipage}
	\end{tabular}
\end{center}

\end{document} % chktex 17
