% chktex-file 46
\documentclass[12pt]{scrartcl}

\PassOptionsToPackage{utf8}{inputenc}
\usepackage{inputenc}
\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage{microtype}

\usepackage{graphicx}
\graphicspath{{images/}}

\usepackage{paralist}
\usepackage{csquotes}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{geometry}
% \geometry{a4paper,body={5.8in,9in}}
\geometry{a4paper}
\renewcommand{\baselinestretch}{1.1}
\usepackage{parskip}
\setlength{\parindent}{0pt}

\usepackage{amsmath, amsfonts, amssymb}
\usepackage{bm}
\usepackage{placeins}
\usepackage{subcaption}

\usepackage{setspace}

\usepackage{hyperref}
\usepackage[nameinlink]{cleveref}
\newcommand{\crefrangeconjunction}{--}

\usepackage[						% use biblatex for bibliography
	backend=bibtex,					% 	- use biber backend (bibtex replacement) or bibtex
	style=numeric,					% 	- use alphabetic (or numeric) bib style
	natbib=true,					% 	- allow natbib commands
	hyperref=true,					% 	- activate hyperref support
	backref=true,					% 	- activate backrefs
	isbn=false,						% 	- don't show isbn tags
	url=false,						% 	- don't show url tags
	doi=false,						% 	- don't show doi tags
	urldate=long,					% 	- display type for dates
	maxnames=3,%
	minnames=1,%
	maxbibnames=5,%
	minbibnames=3,%
	maxcitenames=2,%
	mincitenames=1,%
	sorting=none
]{biblatex}
\bibliography{literature}

\usepackage[inline]{enumitem}
\usepackage{todonotes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\thesisTitle}{Learning to Aggregate on Structured Data}
\newcommand{\thesisSubject}{Master Thesis Proposal \& Work Plan}
\newcommand{\thesisName}{Clemens Damke}
\newcommand{\thesisMail}{cdamke@mail.uni-paderborn.de}
\newcommand{\thesisMatNr}{7011488}
\hypersetup{% setup the hyperref-package options
    pdftitle={\thesisTitle},    %   - title (PDF meta)
    pdfsubject={\thesisSubject},%   - subject (PDF meta)
    pdfauthor={\thesisName},    %   - author (PDF meta)
    plainpages=false,           %   -
    colorlinks=false,           %   - colorize links?
    pdfborder={0 0 0},          %   -
    breaklinks=true,            %   - allow line break inside links
    bookmarksnumbered=true,     %
    bookmarksopen=true          %
}

\begin{document}

\title{\thesisTitle}
\subtitle{\thesisSubject}
\author{{\thesisName}\\\small{Matriculation Number: \thesisMatNr}\\\small{\href{mailto:\thesisMail}{\thesisMail}}}
\date{\today}
\maketitle

\section{Motivation}%
\label{sec:motivation}

Most of the commonly used supervised machine learning techniques assume that instances are represented by $d$-dimensional feature vectors $x_i \in \mathcal{X} = \mathcal{X}_1 \times \cdots \times \mathcal{X}_d$ for which some target value $y_i \in \mathcal{Y}$ should be predicted.
In the regression setting the target domain $\mathcal{Y}$ is continuous, typically $\mathcal{Y} = \mathbb{R}$, whereas $\mathcal{Y}$ is some discrete set of classes in the classification setting.

Since not all data is well-suited for a fixed-dimensional vector representation, approaches that directly consider the structure of the input data might be more appropriate in such cases.
One such case is the class of so-called \textit{learning to aggregate} (LTA) problems as described by \citet{Melnikov2016}.
There the instances are represented by compositions $\bm{c}_i$ of constituents $c_{i,j} \in \bm{c}_i$, i.e.\@ variable-size multisets with $n_i = |\bm{c}_i|$.
The assumption in LTA problems is that for all constituents $c_{i,j}$ a local score $y_{i,j} \in \mathcal{Y}$ is either given or computable.
The set of those local scores should be indicative of the overall score $y_i \in \mathcal{Y}$ of the entire composition $\bm{c}_i$.
LTA problems typically require two subproblems to be solved:
\begin{enumerate}[label=\textbf{\arabic*.}]
	\item \textbf{Aggregation:}
		A variadic aggregation function $A: \mathcal{Y}^{*} \to \mathcal{Y}$ that estimates composite scores has to be learned, i.e.\@ $y_i \approx \hat{y}_i = A(y_{i,1}, \dots, y_{i,n_i})$.
		Typically the aggregation function $A$ should be associative and commutative to fit with the multiset-structure of compositions.
	\item \textbf{Disaggregation:}
		In case the constituent scores $y_{i,j}$ are not given, they have to be derived from a constituent representation, e.g.\ a vector $v_{i,j} \in \mathcal{V}$.
		To learn this derivation function $f: \mathcal{V} \to \mathcal{Y}$, only the constituent vectors ${\{v_{i,j}\}}_{j = 1}^{n_i}$ and the composite score $y_i$ is given.
		Thus the constituent scores $y_{i,j}$ need to be \textit{disaggregated} from $y_i$ in order to learn $f$.
\end{enumerate}
Overall LTA can be understood as the joint problem of learning the aggregation function $A$ and the local score derivation function $f$.

Current LTA approaches only work with multiset inputs.
In practice there is however often some relational structure among the constituents of a composition.
This effectively turns LTA into a graph regression problem.
The goal of this thesis is to look into the question of how aggregation function learning methods might be generalized to the graph setting.

\section{Related Work}%
\label{sec:related-work}

This thesis will be based on two currently mostly separate fields of research:
\begin{enumerate*}[label=\textbf{\arabic*.}]
	\item Learning to Aggregate
	\item Graph classification
\end{enumerate*}.
A short overview of the current state-of-the-art approaches in both fields will be given now.

\subsection{Learning to Aggregate}%
\label{sec:related-work:lta}

Two main approaches to represent the aggregation function in LTA problems have been explored.
The first approach uses \textit{uninorms}~\cite{Melnikov2016} to do so.
There the basic idea is to express composite scores as fuzzy truth assignments $y_i \in [0, 1]$.
Such a composite assignment $y_i$ is modeled as the result of a parameterized logical expression of constituent assignments $y_{i,j} \in [0, 1]$.
As the logical expression that thus effectively aggregates the constituents, a uninorm $U_{\lambda}$ is used.
Depending on the parameter $\lambda$, $U_{\lambda}$ interpolates between t-norms and t-conorms which are continuous generalizations of logical conjunction and disjunction respectively.
\todo{TODO:\@ Details}

Recently \citet{Melnikov2019} have also looked at an alternative class of aggregation function.
Instead of using fuzzy logic to describe score aggregation, \textit{ordered weighted average} (OWA) operators were used.
\todo{TODO:\@ Details}

\subsection{Graph Classification}%
\label{sec:related-work:gc}

As previously mentioned, the addition of relations between constituents turns LTA into a graph regression problem.
Most of the recent research in the field of learning from graph structured data however focused on the closely related graph classification problem.
Since many ideas from the classification setting are also applicable in the regression setting, a brief overview of those ideas is given.

At a high level graph classification methods can be taxonomized into two main families:
\begin{enumerate}[label=\textbf{\arabic*.}]
	\item \textbf{Vector representation approaches:}
		One way to tackle the graph classification problem is to map an input graph $G$ to a vectorial representation.
		This can be done
		\begin{enumerate*}
			\item by either handpicking global graph features like vertex/edge count, degree distribution or graph diameter,
			\item via a graph embedding algorithm like node2vec~\cite{Grover2016}, sub2vec~\cite{Adhikari2018} or graph2vec~\cite{Narayanan2017},
			\item implicitly by using a graph kernel that computes the structural similarity between graphs, e.g.\ the Weisfeiler-Lehman kernel~\cite{Shervashidze2011} or the multiscale Laplacian graph kernel~\cite{Kondor2016}
		\end{enumerate*}.
		Graphs can then be classified via any classification algorithm that works with vectors and/or kernels.
	\item \textbf{Graph neural networks:}
		An alternative approach is to adapt neural networks to graph inputs.
		The notion of a \textit{graph neural network} (GNN) was first introduced by \citet{Gori2005}.\@
		There a message-passing architecture is used to iteratively propagate vertex information to neighbors until a fixed point is reached.
		This process is computationally expensive.

		Recently the class of \textit{convolutional graph neural networks} (ConvGNNs) gained traction.
		It generalizes the convolution operator that is used by \textit{convolutional neural networks} (CNNs) to graphs.
		There are two main variants of ConvGNNs:
		\begin{enumerate}[label=\textbf{\alph*)}] % chktex 9 chktex 10
			\item \textbf{Spatial variant:}
				Standard CNNs use a convolution operator that is applied to a grid graph with diagonal edges.
				Spatial ConvGNNs directly generalize this idea by defining a more flexible convolution that aggregates the local neighborhood of each vertex.
				Spatial graph convolution aggregates the direct neighborhood of each vertex where each vertex $v_i$ is typically described by a feature vector $x_i \in \mathbb{R}^k$.
				This process returns a new aggregate feature vector $x'_i$.
				By stacking multiple convolution layers, features from indirect neighbors become part of the aggregate.
				Analogous to how CNNs learn kernel matrices, spatial ConvGNNs learn vertex neighborhood feature aggregation functions.
				This approach shares similarities with the previously mentioned message-passing architecture~\cite{Gori2005} with the major different being that the number of message-passing, i.e.\ convolution, steps is fixed.
			\item \textbf{Spectral variant:}
				Motivated by the theoretical foundation provided by spectral graph theory, spectral ConvGNNs learn a filter function $\hat{g}$ that is applied in the Fourier domain of a graph.
				Based on the convolutional theorem, a convolution operator can not only be expressed in terms of the neighborhood of vertices but also as a filter on the eigenvectors of a graph's Laplacian.
				Formally this can be expressed as
				\begin{align*}
					g *_G \bm{x} = U \hat{g}(\Lambda) U^{\top} \bm{x}
				\end{align*}
				where $\bm{x} \in \mathbb{R}^{n \times k}$ is the matrix of all vertex features and $U \Lambda U^{\top}$ is the eigendecomposition of the graph Laplacian $L_G$.
				In the previously described spatial ConvGNN variant, $g$ is learned directly in form of the feature aggregation function with a neighborhood locality constraint.
				In the spectral variant however the Fourier-transformed filter $\hat{g}$ is learned.
				Intuitively this means that vertex features $x_i$ are not aggregated with the features of their their direct neighborhood but with the features of vertices with which they share certain structural similarities.
				This allows spectral ConvGNNs to incorporate the global graph structure at the cost of having to perform the computationally expensive decomposition of every input graph's Laplacian.

				To tackle the performance impact of the spectral convolution approach, various simplifications of the filter $\hat{g}$ have been proposed.
				The so-called \textit{graph convolutional network}~(GCN) does this by reducing the expressivity of the filter.
				Then $\hat{g}$ can be applied to the Laplacian directly via $g *_G \bm{x} = \hat{g}(L_G) \bm{x}$ which saves the decomposition costs.
				This simplification of $\hat{g}$ implicitly causes its Fourier inverse $g$ to be locality constrained just like in the spatial ConvGNN variant.
				Therefore the spectral GCN approach also allows for a spatial/message-passing interpretation.
				Recently various variants of GCN have been proposed that apply additional approximations to improve performance in certain scenarios.
		\end{enumerate}
		After the application of either spatial or spectral graph convolutions, each vertex will have an updated aggregate feature vector.
		To obtain an aggregate score for the entire graph, a graph pooling layer is applied.
		A primitive pooling approach is to simply merge the vertex features via a fixed aggregation function like $\min$, $\max$ or $+$.
		Better results can be obtained by learning the pooling aggregation function.
		Two notable graph pooling approaches are SortPooling and Self-Attention Pooling.
		SortPooling works by determining a top-k ordering of the vertices and then aggregates the remaining fixed-size vertex features via a standard multilayer perceptron (MLP).
\end{enumerate}

\section{Goals}%
\label{sec:goals}

\subsection{Required Goals}%
\label{sec:goals:req}

\subsection{Optional Goals}%
\label{sec:goals:opt}

\section{Approach}%
\label{sec:approach}

\section{Preliminary Document Structure}%
\label{sec:doc-structure}

\begin{enumerate}
	\item Introduction
	\item \dots
\end{enumerate}

\section{Time-Schedule}%
\label{sec:schedule}

\begin{figure}[!ht]
	\centering
	%\includegraphics[width=.9\textwidth]{timeschedule}
	\caption{Sketch of the time schedule for the work on the thesis}\label{fig:schedule}
\end{figure}

% % %
\newpage
{%
\renewcommand{\bibfont}{\normalfont\small}
\setlength{\biblabelsep}{5pt}
\setlength{\bibitemsep}{0.5\baselineskip plus 0.5\baselineskip} % chktex 1
\setcounter{biburllcpenalty}{9000}
\setcounter{biburlucpenalty}{9999}
\printbibliography%
}

\vspace{6cm}

\begin{center}
	\begin{tabular}{l p{0.1\textwidth} r}
		\cline{1-1} \cline{3-3}
		\begin{minipage}[t]{0.4\textwidth}
			\centering
			\vspace{0cm}Supervisor
		\end{minipage}
		&
		\begin{minipage}[t]{0.2\textwidth}
		\end{minipage}
		&
		\begin{minipage}[t]{0.4\textwidth}
			\centering
			\vspace{0cm}Student
		\end{minipage}
	\end{tabular}
\end{center}

\end{document}
