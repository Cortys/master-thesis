% chktex-file 46
\documentclass[12pt]{scrartcl}

\PassOptionsToPackage{utf8}{inputenc}
\usepackage{inputenc}
\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage{microtype}

\usepackage{graphicx}
\graphicspath{{images/}}

\usepackage{paralist}
\usepackage{csquotes}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{geometry}
% \geometry{a4paper,body={5.8in,9in}}
\geometry{a4paper}
\renewcommand{\baselinestretch}{1.1}
\usepackage{parskip}
\setlength{\parindent}{0pt}

\usepackage{amsmath, amsfonts, amssymb}
\usepackage{bm}
\usepackage{placeins}
\usepackage{subcaption}

\usepackage{setspace}

\usepackage{hyperref}
\usepackage[nameinlink]{cleveref}
\newcommand{\crefrangeconjunction}{--}

\usepackage[						% use biblatex for bibliography
	backend=bibtex,					% 	- use biber backend (bibtex replacement) or bibtex
	style=numeric,					% 	- use alphabetic (or numeric) bib style
	natbib=true,					% 	- allow natbib commands
	hyperref=true,					% 	- activate hyperref support
	backref=true,					% 	- activate backrefs
	isbn=false,						% 	- don't show isbn tags
	url=false,						% 	- don't show url tags
	doi=false,						% 	- don't show doi tags
	urldate=long,					% 	- display type for dates
	maxnames=3,%
	minnames=1,%
	maxbibnames=5,%
	minbibnames=3,%
	maxcitenames=2,%
	mincitenames=1,%
	sorting=none
]{biblatex}
\bibliography{literature}

\usepackage[inline]{enumitem}
\usepackage{todonotes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\thesisTitle}{Learning to Aggregate on Structured Data}
\newcommand{\thesisSubject}{Master Thesis Proposal \& Work Plan}
\newcommand{\thesisName}{Clemens Damke}
\newcommand{\thesisMail}{cdamke@mail.uni-paderborn.de}
\newcommand{\thesisMatNr}{7011488}
\hypersetup{% setup the hyperref-package options
    pdftitle={\thesisTitle},    %   - title (PDF meta)
    pdfsubject={\thesisSubject},%   - subject (PDF meta)
    pdfauthor={\thesisName},    %   - author (PDF meta)
    plainpages=false,           %   -
    colorlinks=false,           %   - colorize links?
    pdfborder={0 0 0},          %   -
    breaklinks=true,            %   - allow line break inside links
    bookmarksnumbered=true,     %
    bookmarksopen=true          %
}

\begin{document}

\title{\thesisTitle}
\subtitle{\thesisSubject}
\author{{\thesisName}\\\small{Matriculation Number: \thesisMatNr}\\\small{\href{mailto:\thesisMail}{\thesisMail}}}
\date{\today}
\maketitle

\section{Motivation}%
\label{sec:motivation}

Most of the commonly used supervised machine learning techniques assume that instances are represented by $d$-dimensional feature vectors $x_i \in \mathcal{X} = \mathcal{X}_1 \times \cdots \times \mathcal{X}_d$ for which some target value $y_i \in \mathcal{Y}$ should be predicted.
In the regression setting the target domain $\mathcal{Y}$ is continuous, typically $\mathcal{Y} = \mathbb{R}$, whereas $\mathcal{Y}$ is some discrete set of classes in the classification setting.

Since not all data is well-suited for a fixed-dimensional vector representation, approaches that directly consider the structure of the input data might be more appropriate in such cases.
One such case is the class of so-called \textit{learning to aggregate} (LTA) problems as described by \citet{Melnikov2016}.
There the instances are represented by compositions $\bm{c}_i$ of constituents $c_{i,j} \in \bm{c}_i$, i.e.\@ variable-size multisets with $n_i = |\bm{c}_i|$.
The assumption in LTA problems is that for all constituents $c_{i,j}$ a local score $y_{i,j} \in \mathcal{Y}$ is either given or computable.
The set of those local scores should be indicative of the overall score $y_i \in \mathcal{Y}$ of the entire composition $\bm{c}_i$.
LTA problems typically require two subproblems to be solved:
\begin{enumerate}[label=\textbf{\arabic*.}]
	\item \textbf{Aggregation:}
		A variadic aggregation function $A: \mathcal{Y}^{*} \to \mathcal{Y}$ that estimates composite scores has to be learned, i.e.\@ $y_i \approx \hat{y}_i = A(y_{i,1}, \dots, y_{i,n_i})$.
		Typically the aggregation function $A$ should be associative and commutative to fit with the multiset-structure of compositions.
	\item \textbf{Disaggregation:}
		In case the constituent scores $y_{i,j}$ are not given, they have to be derived from a constituent representation, e.g.\ a vector $v_{i,j} \in \mathcal{V}$.
		To learn this derivation function $f: \mathcal{V} \to \mathcal{Y}$, only the constituent vectors ${\{v_{i,j}\}}_{j = 1}^{n_i}$ and the composite score $y_i$ is given.
		Thus the constituent scores $y_{i,j}$ need to be \textit{disaggregated} from $y_i$ in order to learn $f$.
\end{enumerate}
Overall LTA can be understood as the joint problem of learning the aggregation function $A$ and the local score derivation function $f$.

Current LTA approaches only work with multiset inputs.
In practice there is however often some relational structure among the constituents of a composition.
This effectively turns LTA into a graph regression problem.
The goal of this thesis is to look into the question of how aggregation function learning methods might be generalized to the graph setting.

\section{Related Work}%
\label{sec:related-work}

This thesis will be based on two currently mostly separate fields of research:
\begin{enumerate*}[label=\textbf{\arabic*.}]
	\item Learning to Aggregate
	\item Graph classification \& regression
\end{enumerate*}.
A short overview of the current state-of-the-art approaches in both fields will be given now.

\subsection{Learning to Aggregate}%
\label{sec:related-work:lta}

Two main approaches to represent the aggregation function in LTA problems have been explored.
The first approach uses \textit{uninorms}~\cite{Melnikov2016} to do so.
There the basic idea is to express composite scores as fuzzy truth assignments $y_i \in [0, 1]$.
Such a composite assignment $y_i$ is modeled as the result of a parameterized logical expression of constituent assignments $y_{i,j} \in [0, 1]$.
As the logical expression that thus effectively aggregates the constituents, a uninorm $U_{\lambda}$ is used.
Depending on the parameter $\lambda$, $U_{\lambda}$ interpolates between t-norms and t-conorms which are continuous generalizations of logical conjunction and disjunction respectively.

Recently \citet{Melnikov2019} have also looked at an alternative class of aggregation function.
Instead of using fuzzy logic to describe score aggregation, \textit{ordered weighted average} (OWA) operators were used.
OWA aggregators work by sorting the input scores and then weighting them based on their sort position, i.e.\ %
\begin{align*}
	A_{\lambda}(y_1, \dots, y_n) := \sum_{i = 1}^n \lambda_i y_{\pi(i)},
\end{align*}
where $\lambda$ is a weight vector with ${\|\lambda\|}_1 = 1$ and $\pi$ is a sorting permutation of the input scores. % chktex 21
To deal with varying composite sizes $n$ the weights $\lambda_i$ are interpolated using a \textit{basic unit interval monotone} (BUM) function $q: [0, 1] \to [0, 1]$.
It is used by normalizing every constituent's position $\pi(i)$ to the unit interval via $\frac{\pi(i)}{n}$.
The BUM function $q$ is then used to interpolate a weight for any normalized sort position.
Therefore the learning objective boils down to optimizing the shape of $q$.
The details of this are left out here.

\subsection{Graph Classification}%
\label{sec:related-work:gc}

As previously mentioned, the addition of relations between constituents turns LTA into a graph regression problem.
Most of the recent research in the field of learning from graph structured data however focused on the closely related graph classification problem.
Since many ideas from the classification setting are also useful in the regression setting, a brief overview of those ideas is given.

At a high level graph classification methods can be taxonomized into two main families:
\begin{enumerate}[label=\textbf{\arabic*.}]
	\item \textbf{Vector representation approaches:}
		One way to tackle the graph classification problem is to map an input graph $G$ to a vectorial representation.
		This can be done
		\begin{enumerate*}
			\item by either handpicking global graph features like vertex/edge count, degree distribution or graph diameter,
			\item via a graph embedding algorithm like node2vec~\cite{Grover2016}, sub2vec~\cite{Adhikari2018} or graph2vec~\cite{Narayanan2017},
			\item implicitly by using a graph kernel that computes the structural similarity between graphs, e.g.\ the Weisfeiler-Lehman kernel~\cite{Shervashidze2011} or the multiscale Laplacian graph kernel~\cite{Kondor2016}
		\end{enumerate*}.
		Graphs can then be classified via any classification algorithm that works with vectors and/or kernels.
	\item \textbf{Graph neural networks:}
		An alternative approach is to adapt neural networks to graph inputs.
		The notion of a \textit{graph neural network} (GNN) was first introduced by \citet{Gori2005}.\@
		There a message-passing architecture is used to iteratively propagate vertex information to neighbors until a fixed point is reached.
		This process is computationally expensive.

		Recently the class of \textit{convolutional graph neural networks} (ConvGNNs) gained traction.
		It generalizes the convolution operator that is used by \textit{convolutional neural networks} (CNNs) to graphs.
		There are two main variants of ConvGNNs:
		\begin{enumerate}[label=\textbf{\alph*)}] % chktex 9 chktex 10
			\item \textbf{Spatial variant:}
				Standard CNNs use a convolution operator that is applied to a grid graph with diagonal edges.
				Spatial ConvGNNs~\cite{Micheli2009} directly generalize this idea by defining a more flexible convolution that aggregates the local neighborhood of each vertex.
				Spatial graph convolution aggregates the direct neighborhood of each vertex where each vertex $v_i$ is typically described by a feature vector $x_i \in \mathbb{R}^d$.
				This process returns a new aggregate feature vector $x'_i$.
				By stacking multiple convolution layers, features from indirect neighbors become part of the aggregate.
				Analogous to how CNNs learn kernel matrices, spatial ConvGNNs learn vertex neighborhood feature aggregation functions.
				This approach shares similarities with the previously mentioned message-passing architecture~\cite{Gori2005} with the major different being that the number of message-passing, i.e.\ convolution, steps is fixed.
			\item \textbf{Spectral variant:}
				Motivated by the theoretical foundation provided by spectral graph theory~\cite{Shuman2013}, spectral ConvGNNs~\cite{Bruna2013} learn a filter function $\hat{g}$ that is applied in the Fourier domain of a graph.
				Based on the convolutional theorem, a convolution operator can not only be expressed in terms of the neighborhood of vertices but also as a filter on the eigenvectors of a graph's Laplacian.
				Formally this can be expressed as
				\begin{align*}
					g *_G \bm{x} = U \hat{g}(\Lambda) U^{\top} \bm{x}
				\end{align*}
				where $\bm{x} \in \mathbb{R}^{n \times k}$ is the matrix of all vertex features and $U \Lambda U^{\top}$ is the eigendecomposition of the graph Laplacian $L_G$.
				In the previously described spatial ConvGNN variant, $g$ is learned directly in form of the feature aggregation function with a neighborhood locality constraint.
				In the spectral variant however the Fourier-transformed filter $\hat{g}$ is learned.
				Intuitively this means that vertex features $x_i$ are not aggregated with the features of their their direct neighborhood but with the features of vertices with which they share certain structural similarities.
				This allows spectral ConvGNNs to incorporate the global graph structure at the cost of having to perform the computationally expensive decomposition of every input graph's Laplacian.

				To tackle the performance impact of the spectral convolution approach, various simplifications of the filter $\hat{g}$ have been proposed.
				The so-called \textit{graph convolutional network}~(GCN)~\cite{Kipf2017} does this by reducing the expressivity of the filter.
				Then $\hat{g}$ can be applied to the Laplacian directly via $g *_G \bm{x} = \hat{g}(L_G) \bm{x}$ which saves the decomposition costs.
				This simplification of $\hat{g}$ implicitly causes its Fourier inverse $g$ to be locality constrained just like in the spatial ConvGNN variant.
				Therefore the spectral GCN approach also allows for a spatial/message-passing interpretation.
				Recently various variants and extensions of GCNs~\cite{Hamilton2017}\cite{Chen2018}\cite{Chen2017}\cite{Chiang2019}\cite{Du2017} have been proposed that apply additional approximations and sampling methods to improve the runtime as well as the accuracy in certain scenarios.

				One notable extension of GCNs are the so-called \textit{adadptive graph convolutional networks} (AGCNs)~\cite{Li2018}.
				AGCNs do not use a fixed Laplacian $L$ for each input but instead learn a so-called residual Laplacian $L_\mathit{res}$.
				The residual Laplacian is constructed by learning a vertex similarity kernel $\mathbb{G}_{x_i,x_j}$ which is used to determine the residual edge weights between vertices.
				By overlaying the learned residual Laplacian $L_\mathit{res}$ on top of the static intrinsic Laplacian $L$, similar nodes with a large spatial distance in $L$ become connected.
				This can improve the accuracy especially if a locality constrained spectral filter $\hat{g}$ is used.
		\end{enumerate}
		After the application of either spatial or spectral graph convolutions, each vertex will have an updated aggregate feature vector.
		To obtain an aggregate score for the entire graph, a graph pooling layer is applied.
		A primitive pooling approach is to simply merge the vertex features via a fixed aggregation function like $\min$, $\max$ or $+$.
		Better results can be obtained by learning the pooling aggregation function.
		Two notable graph pooling approaches are \textit{SortPooling}~\cite{Zhang2018} and \textit{Self-Attention Pooling}~\cite{Lee2019}.
		\begin{enumerate}
			\item SortPooling assumes that the vertex feature vectors are obtained using a stack of spectral convolution layers that learn filters on the random walk Laplacian $L = I - D^{-1} A$.
				This assumption makes it possible to interpret the outputs of the graph convolution layers as a continuous generalization of \textit{Weisfeiler-Lehman} (WL) colors.
				WL colors are lexicographically sortable signatures that encode the structural role of vertices; they can be used for efficient graph isomorphism falsification~\cite{Weisfeiler1968}.
				Using this interpretation of the vertex feature vectors, SortPooling determines a top-$k$ ranking of the vertices' WL colors and then aggregates the thus selected fixed-size vertex feature subset via a standard \textit{multilayer perceptron} (MLP).
			\item Self-Attention Pooling adds an additional graph convolution layer after an arbitrary ConvGNN.\@
				The added layer takes the aggregated vertex feature vectors as input and outputs a score for each vertex.
				This score is used to determine a top-$k$ vertex ranking.
				Unlike SortPooling the feature vectors of the resulting subset are first added up.
				The resulting aggregate graph feature vector is then fed into a MLP to obtain a class.
		\end{enumerate}
\end{enumerate}

\subsection{Graph Regression}%
\label{sec:related-work:gr}

An overview of the recent work regarding graph classification was just given.
The generalization of LTA to the graph domain is however better described as a graph regression problem since the predicted composite scores are continuous.
As there currently are relatively few graph regression approaches, only few references can be provided here.

Graph regression problems have for the most part only been discussed in the context of specific domain problems.
One of those domains is the prediction of chemical toxicity, typically the $\text{LD}_{50}$ or $\text{LC}_{50}$ doses which measure the smallest amount of a given chemical that is lethal for at least $50\%$ of tested subjects.
The following approaches could serve as inspiration, baselines and/or sources of training data:
\begin{itemize}
	\item \textit{RASAR}~\cite{Luechtefeld2018} uses a logistic regression model.
		It is trained with manually chosen features that are given for training molecule graphs.
		To predict the toxicity of a new chemical, $k$-means is used to average the features of chemicals with similar molecule graphs in the training set.
		The structural similarity between molecule graphs is measured by the Tanimoto coefficient of their chemical fingerprints.
		Those fingerprints encode the presence or absence of certain meaningful chemical substructures.
		The implementation and used dataset is unfortunately not publicly available because it is part of a commercial product\footnote{ToxTrack Inc., \url{https://toxtrack.com/}}.
	\item \textit{ProTox}~\cite{Drwal2014}\cite{Banerjee2018} is another toxicity prediction approach.
		It works similarly to RASAR but leaves out the logistic regression step and instead directly aggregates the toxicity of similar chemicals via $k$-means.
		ProTox uses the freely available \textit{SuperToxic} dataset~\cite{Schmidt2009}.
		While the implementation is again not publicly available, the trained model can be freely queried online\footnote{ProTox-II, \url{http://tox.charite.de/protox_II/}}.
	\item The \textit{Toxicity Estimation Software Tools}~(T.E.S.T.)\footnote{\url{https://www.epa.gov/chemical-research/toxicity-estimation-software-tool-test}} is a free collection of models for toxicity prediction.
		Like RASAR and ProTox it includes similarity-based prediction models but also linear regression and decision tree models that are trained on vector representations of chemicals.
		The \textit{ChemIDplus}\footnote{\url{https://chem.nlm.nih.gov/chemidplus/}} database was used for training.
\end{itemize}

Recently first toxicity prediction approaches using ConvGNNs have been published.
The previously mentioned AGCN~\cite{Li2018} was in fact evaluated on a discrete toxicity classification problem;
this could serve as a starting point for a continuous graph regression method.
\citet{Pope2018} describe how functional groups of toxic chemicals can be learned using GCNs;
there a functional group refers to a relevant subgraph of a molecule that can be interpreted as a toxicity indicator.

\section{Goals}%
\label{sec:goals}

As mentioned in \cref{sec:motivation}, the overall goal of the thesis is to extend the LTA approach described in \cref{sec:related-work:lta} to the graph setting.
Since this goal strongly overlaps with the work in graph classification described in \cref{sec:related-work:gc}, both branches of research should be combined.
The practical applications described in \cref{sec:related-work:gr} could be used for comparison and as a potential source of a graph regression test dataset.
Now follows a concrete list of required and optional goals that should be achieved.

\subsection{Required Goals}%
\label{sec:goals:req}

\begin{enumerate}[label=\textbf{\arabic*.}]
	\item \textbf{Aggregation:}
		Define a class of aggregation functions that takes vertex scores as its input.
		It should fulfill the role of the uninorms and OWA functions in the existing LTA papers (see \cref{sec:related-work:lta}).
	\item \textbf{Disaggregation:}
		Describe how vertex features should be used to compute a constituent score that will be combined by the aggregation function.
	\item \textbf{Learning:}
		Combine the chosen aggregation and disaggregation methods into a joint LTA model with a suitable loss function.
	\item \textbf{Implementation:}
		A reference implementation for the designed LTA model should be developed.
	\item \textbf{Evaluation:}
		The reference implementation should be used to evaluate the model on at least one test dataset.
		Due to the scarcity of graph regression datasets, the used test data might be randomly generated.
		Generated dataset should be obtained using a variety of representative deterministic graph scoring measures to check which kinds of local and global structural features are learnable;
		some examples of measures that might be part of a graph scoring functions are the count of $k$-cliques in a graph, the graph diameter, the presence of cycles or various properties of a graph's degree distribution.
		The results should be compared with the results of at least one other approach for learning graph scores, e.g.\ a ConvCNN with a fixed aggregation function instead of a learned one.
\end{enumerate}

\subsection{Optional Goals}%
\label{sec:goals:opt}

If possible, the previous goals could be extended.
For each of the previous five goal categories, optional additional ideas are listed below.
\begin{enumerate}[label=\textbf{\arabic*.}]
	\item \textbf{Aggregation:}
		Multiple classes of aggregation functions could be considered and compared.
	\item \textbf{Disaggregation:}
		As describen in \cref{sec:related-work:gc} there are multiple methods to learn vertex scores.
		Depending on the chosen overall approach, a few of those methods might be suitable to be combined with an aggregation function learner.
		Those suitable combinations could be evaluated and compared.
	\item \textbf{Learning:}
		The joint model of aggregation and disaggregation function learning can most likely be optimized using various algorithms.
		Those algorithms could be compared.
		In case their runtime performance is impractical in certain situations, approximations and simplifications of the model could be considered.
	\item \textbf{Implementation:}
		The chosen approach should ideally align well with the practical constraints of implementation.
		It should be suitable for an implementation on modern GPUs and/or distributed cluster environments.
	\item \textbf{Evaluation:}
		Not only generated data but also real datasets should be used for evaluation (see \cref{sec:related-work:gr}).
\end{enumerate}

\section{Approach}%
\label{sec:approach}



\section{Preliminary Document Structure}%
\label{sec:doc-structure}

\begin{enumerate}
	\item Introduction
	\item
	\item Evaluation
	\item Conclusion
	\item Literature
	\item Appendix
\end{enumerate}

\section{Time-Schedule}%
\label{sec:schedule}

\begin{figure}[!ht]
	\centering
	%\includegraphics[width=.9\textwidth]{timeschedule}
	\caption{Sketch of the time schedule for the work on the thesis}\label{fig:schedule}
\end{figure}

% % %
\newpage
{%
\renewcommand{\bibfont}{\normalfont\small}
\setlength{\biblabelsep}{5pt}
\setlength{\bibitemsep}{0.5\baselineskip plus 0.5\baselineskip} % chktex 1
\setcounter{biburllcpenalty}{9000}
\setcounter{biburlucpenalty}{9999}
\printbibliography%
}

\vspace{6cm}

\begin{center}
	\begin{tabular}{l p{0.1\textwidth} r}
		\cline{1-1} \cline{3-3}
		\begin{minipage}[t]{0.4\textwidth}
			\centering
			\vspace{0cm}Supervisor
		\end{minipage}
		&
		\begin{minipage}[t]{0.2\textwidth}
		\end{minipage}
		&
		\begin{minipage}[t]{0.4\textwidth}
			\centering
			\vspace{0cm}Student
		\end{minipage}
	\end{tabular}
\end{center}

\end{document} % chktex 17
